{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":67079,"databundleVersionId":7452256,"sourceType":"competition"}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>Recipe for Rating: Predict Food Ratings using ML - MLP Project</h1>\n\n**Name:** Archit Handa | **Roll No.:** 22F2000744 | **Student Email:** [22f2000744@ds.study.iitm.ac.in](mailto:22f2000744@ds.study.iitm.ac.in)\n\n---\n\n### **_Objective_**\n>Create a Machine Learning model using scikit-learn to predict ratings for food recipes based on the various features\n\n### **_Libraries_**\n> #### **Data Analysis**\n   >- Pandas    \n   >- Numpy\n   >- Scipy\n   \n> #### **Data Visualization**\n   >- Matplotlib\n   >- Seaborn\n\n> #### **Modeling**\n   >- Scikit-Learn\n   >- XGBoost\n   \n> #### **Text Analysis - Inbuilt Libraries**\n   >- Regex (re)\n   >- HTML (html)\n   >- UnicodeData (unicodedata)","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents\n\n- 1. [Importing Libraries](#Libraries)\n\n- 2. [Loading Data](#Load-Data)\n\n- 3. [Exploratory Data Analysis](#EDA)\n\n    - 3.1 [Statistical Summaries](#Stat-Summary)\n    \n    - 3.2 [Hypothesis Testing](#Hypothesis)\n    \n    - 3.3 [Target Variable](#Target)\n    \n    - 3.4 [Missina and Duplicate Values](#Missing-Duplicate)\n    \n    - 3.5 [Numerical Features](#Numeric)\n    \n    - 3.6 [Date-Time Feature](#Date-Time)\n    \n    - 3.7 [Correlation Matrices](#Corr-Matrix)\n    \n    - 3.8 [EDA Conclusions](#EDA-Conclusion)\n\n- 4. [Preprocessing](#Preprocessing)\n    \n    - 4.1 [Numerical Features](#Preprocessing-Numerical)\n    \n    - 4.2 [DateTime Features](#Preprocessing-DateTime)\n    \n    - 4.3 [Text Features - Latent Semantic Analysis (LSA)](#Preprocessing-Text)\n    \n    - 4.4 [Combined Preprocessor](#Preprocessing-Combined)\n\n- 5. [Model Training and Selection](#Model-Training-Selection)\n\n   - 5.1 [Train-Validation Datasets](#Train-Validation)\n    \n   - 5.2 [Helper Functions](#Helper)\n    \n   - 5.3 [Models](#Model)\n    \n       - 5.3.1 [Logistic Regression](#Logistic-Regression)\n      \n       - 5.3.2 [K-Nearest Neighbors](#KNN)\n      \n       - 5.3.3 [Support Vector Classifier](#SVC)\n       \n       - 5.3.4 [Decision Tree](#DT)\n       \n       - 5.3.5 [Random Forest - Bagging](#Random-Forest)\n       \n       - 5.3.6 [XGBoostClassifier - Boosting](#XGBoost)\n       \n       - 5.3.7 [Multi-Layer Perceptron](#MLP)\n    \n    - 5.4 [Model Comparison](#Model-Compare)\n    \n       - 5.4.1 [Comparing Metrics](#Compare-Metrics)\n      \n       - 5.4.2 [ROC Curves](#ROC-Curve)\n      \n    - 5.5 [Ensemble](#Ensemble)\n    \n       - 5.5.1 [Voting Classifier](#Voting)\n      \n         - 5.5.1.1 [Hard Voting Classifier](#Hard-Voting)\n      \n         - 5.5.1.2 [Soft Voting Classifier](#Soft-Voting)\n      \n       - 5.5.2 [Stacking Classifier](#Stacking)\n    \n    - 5.6 [Model Comparison Part-2](#Model-Compare-2)\n    \n       - 5.6.1 [ROC Curves](#ROC-Curve-2)\n       \n       - 5.6.2 [Confusion and Error Matrices](#Confusion-Error-Matrix)\n\n- 6. [Conclusion](#Conclusion)\n\n    - 6.1 [Final Model and Generating Submission File](#Final-Model)\n    \n    - 6.2 [Closing Remarks and Further Scope](#Close-Scope)","metadata":{}},{"cell_type":"markdown","source":"<a name='Libraries'></a>\n# 1. Importing Libraries","metadata":{}},{"cell_type":"code","source":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport re\nimport html\nimport unicodedata\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:14.413712Z","iopub.execute_input":"2024-04-06T01:59:14.414540Z","iopub.status.idle":"2024-04-06T01:59:16.457431Z","shell.execute_reply.started":"2024-04-06T01:59:14.414487Z","shell.execute_reply":"2024-04-06T01:59:16.456136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting random_state for reproducability\nrandom_state = 42\nnp.random.seed(random_state)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:16.459971Z","iopub.execute_input":"2024-04-06T01:59:16.460961Z","iopub.status.idle":"2024-04-06T01:59:16.467939Z","shell.execute_reply.started":"2024-04-06T01:59:16.460908Z","shell.execute_reply":"2024-04-06T01:59:16.466000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='Load-Data'></a>\n# 2. Loading Data","metadata":{}},{"cell_type":"code","source":"# Load data from CSV files\nPATH = '/kaggle/input/recipe-for-rating-predict-food-ratings-using-ml'\n\ntrain = pd.read_csv(os.path.join(PATH, 'train.csv'))\ntest = pd.read_csv(os.path.join(PATH, 'test.csv'))\nsample = pd.read_csv(os.path.join(PATH, 'sample.csv'))","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:16.469941Z","iopub.execute_input":"2024-04-06T01:59:16.470437Z","iopub.status.idle":"2024-04-06T01:59:16.742633Z","shell.execute_reply.started":"2024-04-06T01:59:16.470389Z","shell.execute_reply":"2024-04-06T01:59:16.741333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preview Train Data\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:16.745368Z","iopub.execute_input":"2024-04-06T01:59:16.745744Z","iopub.status.idle":"2024-04-06T01:59:16.779482Z","shell.execute_reply.started":"2024-04-06T01:59:16.745709Z","shell.execute_reply":"2024-04-06T01:59:16.778594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preview Test Data\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:16.780891Z","iopub.execute_input":"2024-04-06T01:59:16.781412Z","iopub.status.idle":"2024-04-06T01:59:16.798734Z","shell.execute_reply.started":"2024-04-06T01:59:16.781370Z","shell.execute_reply":"2024-04-06T01:59:16.797773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preview Sample submission\nsample.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:16.800068Z","iopub.execute_input":"2024-04-06T01:59:16.800596Z","iopub.status.idle":"2024-04-06T01:59:16.810276Z","shell.execute_reply.started":"2024-04-06T01:59:16.800562Z","shell.execute_reply":"2024-04-06T01:59:16.809013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking shape of datasets\ntrain.shape, test.shape, sample.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:16.812486Z","iopub.execute_input":"2024-04-06T01:59:16.812863Z","iopub.status.idle":"2024-04-06T01:59:16.824953Z","shell.execute_reply.started":"2024-04-06T01:59:16.812833Z","shell.execute_reply":"2024-04-06T01:59:16.823742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking ratio of test set\nprint(f'Test Size Ratio (of Training Data): {test.shape[0] / train.shape[0]:.2f}')\nprint(f'Test Size Ratio (of Overall Data): {test.shape[0] / (train.shape[0] + test.shape[0]):.2f}')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:16.826868Z","iopub.execute_input":"2024-04-06T01:59:16.827346Z","iopub.status.idle":"2024-04-06T01:59:16.836109Z","shell.execute_reply.started":"2024-04-06T01:59:16.827300Z","shell.execute_reply":"2024-04-06T01:59:16.834930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Key Insights:\n- The train dataset composes of **13636 records** with **14 features** and **1 target** variable (_'Rating'_).\n- The test dataset is roughly **one-third** ($1/3$) of the train dataset, while **one-fourth** ($1/4$) of the entire dataset.\n- The 'Rating' variable for the test set needs to be predicted and, hence, is missing.\n- Submission needs to be made with 2 columns ('ID' and 'Rating')\n\n**Note:** Upon enquiry with the instructors, the 'ID' column of sample does not correspond with the 'ID' column of the dataset, and is just a row index starting from 1","metadata":{}},{"cell_type":"markdown","source":"<a name='EDA'></a>\n# 3. Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"<a name='Stat-Summary'></a>\n## 3.1 Statistical Summaries","metadata":{}},{"cell_type":"code","source":"# Checking data type of each column\ntrain.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:16.837194Z","iopub.execute_input":"2024-04-06T01:59:16.837574Z","iopub.status.idle":"2024-04-06T01:59:16.961811Z","shell.execute_reply.started":"2024-04-06T01:59:16.837541Z","shell.execute_reply":"2024-04-06T01:59:16.960895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Statistical Summary for Train data\ntrain.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:16.967453Z","iopub.execute_input":"2024-04-06T01:59:16.967915Z","iopub.status.idle":"2024-04-06T01:59:17.079837Z","shell.execute_reply.started":"2024-04-06T01:59:16.967877Z","shell.execute_reply":"2024-04-06T01:59:17.078869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Unique entries per column\ntrain.nunique()\n\n# Total no. of rows: 13636","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:17.081382Z","iopub.execute_input":"2024-04-06T01:59:17.082457Z","iopub.status.idle":"2024-04-06T01:59:17.115491Z","shell.execute_reply.started":"2024-04-06T01:59:17.082416Z","shell.execute_reply":"2024-04-06T01:59:17.113982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Number-type Columns: {list(train.select_dtypes(include=\"number\").columns)}')\nprint(f'Object-type Columns: {list(train.select_dtypes(exclude=\"number\").columns)}')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:17.117204Z","iopub.execute_input":"2024-04-06T01:59:17.117779Z","iopub.status.idle":"2024-04-06T01:59:17.128847Z","shell.execute_reply.started":"2024-04-06T01:59:17.117725Z","shell.execute_reply":"2024-04-06T01:59:17.127319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Feature Types:**\n\n> **Numerical:**\n>   - ID\n>   - RecipeNumber\n>   - UserReputation\n>   - CreationTimestamp\n>   - ReplyCount\n>   - ThumbsUpCount\n>   - ThumbsDownCount\n>   - BestScore\n\n\n> **Categorical (Nominal):**\n>   - RecipeCode\n>   - RecipeName\n>   - CommentID\n>   - UserID\n>   - UserName\n    \n> **Text:**\n>   - RecipeName\n>   - Recipe_Review\n    \n> **Target:**\n>   - Rating: Discrete (0 or 1-5), Categorical-Ordinal\n\n<br>\n\n#### **Note on RecipeCode and RecipeNumber**\n>Though, RecipeCode is numeric in nature, it has just _100_ unique values within the range of _386_-_191775_; thus, it seems more appropriate to be a numeric identifier for the recipe than representing any numerical measure.\n\n>On the other, RecipeNumber also is a numeric quantity with _100_ unique values (range _1_-_100_); however, it represents the internal ranking for the recipe on the top-100 recipes list, and hence, is classified as a Numerical feature.","metadata":{}},{"cell_type":"markdown","source":"#### **Further inferences from the summaries:**\n\n- RecipeNumber, RecipeCode, and RecipeName have _100_ unique values each; seems to be a one-one relation between all of three features\n\n- CommentID has _13636_ unique values (same as number of records), hence might not be informative in this state; may require feature engineering\n\n- UserID and UserName have similar unique counts (_10783_ and _10628_ respectively) hinting towards a one-one relation\n\n- More information can be extracted from CreationTimestamp by converting into year, month, day etc.\n\n- ReplyCount has extremely low unique count (_4_) with a near-zero mean of _0.014520_ and low standard deviation of _0.137323_; may require feature engineering\n\n<br>\n\nThese inferences/hypotheses should be tested ahead.","metadata":{}},{"cell_type":"markdown","source":"<a name='Hypothesis'></a>\n## 3.2 Hypothesis Testing","metadata":{}},{"cell_type":"markdown","source":"### 3.2.1 Hypothesis 1: One-One Relation between RecipeNumber, RecipeCode, RecipeName\n\nAll 3 features have exact 100 distinct values signalling a one-one relation","metadata":{}},{"cell_type":"code","source":"# Function to check one-one relation between 2 columns of a DataFrame\ndef is_one_one(df, col1, col2):\n    return df.groupby(col1)[col2].nunique().max() == 1","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:17.130489Z","iopub.execute_input":"2024-04-06T01:59:17.130895Z","iopub.status.idle":"2024-04-06T01:59:17.139049Z","shell.execute_reply.started":"2024-04-06T01:59:17.130858Z","shell.execute_reply":"2024-04-06T01:59:17.137845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"One-one RecipeNumber and RecipeCode: {is_one_one(train, 'RecipeNumber', 'RecipeCode')}\")\nprint(f\"One-one RecipeNumber and RecipeName: {is_one_one(train, 'RecipeNumber', 'RecipeName')}\")\nprint(f\"One-one RecipeName and RecipeCode:   {is_one_one(train, 'RecipeName', 'RecipeCode')}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:17.140505Z","iopub.execute_input":"2024-04-06T01:59:17.141572Z","iopub.status.idle":"2024-04-06T01:59:17.162086Z","shell.execute_reply.started":"2024-04-06T01:59:17.141516Z","shell.execute_reply":"2024-04-06T01:59:17.160909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Hypothesis 1**\n\n> **Confirmed:** As suspected, the 3 features are linked to each other.\n\n> **Conclusion:**\n> - Treat RecipeNumber as a Numerical Feature as it contains information regarding internal rank for a recipe\n> - Treat RecipeName as Text Feature since it contains sentiment words, like 'Simple', 'Basic', 'First-Place'\n> - Drop RecipeCode as it is just an identifier for each recipe, and OneHotEncoding it will create 100 sparse features","metadata":{}},{"cell_type":"markdown","source":"### 3.2.2 Hypothesis 2: CommentID is unique\n\nWhile there are 13636 values signalling CommentID being unique feature, its values hint at it being a composite feature (joined by '_')","metadata":{}},{"cell_type":"code","source":"# Extracting individual component from CommentID feature\ncid = pd.DataFrame()\ncid['CommentID'] = train['CommentID'].copy()\n\nn_components = len(train.loc[0, 'CommentID'].split('_'))\nfor i in range(n_components):\n    cid[f'Part_{i + 1}'] = cid['CommentID'].apply(lambda x: x.split('_')[i].strip())\n\ncid","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:17.163776Z","iopub.execute_input":"2024-04-06T01:59:17.164176Z","iopub.status.idle":"2024-04-06T01:59:17.250534Z","shell.execute_reply.started":"2024-04-06T01:59:17.164139Z","shell.execute_reply":"2024-04-06T01:59:17.249379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Number of Distinct Values (Part-1): {cid[\"Part_1\"].nunique()}')\nprint(f'Number of Distinct Values (Part-2): {cid[\"Part_2\"].nunique()}')\nprint(f'Number of Distinct Values (Part-3): {cid[\"Part_3\"].nunique()}')\nprint(f'Number of Distinct Values (Part-4): {cid[\"Part_4\"].nunique()}')\nprint(f'Number of Distinct Values (Part-5): {cid[\"Part_5\"].nunique()}')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:17.252252Z","iopub.execute_input":"2024-04-06T01:59:17.253067Z","iopub.status.idle":"2024-04-06T01:59:17.274246Z","shell.execute_reply.started":"2024-04-06T01:59:17.253021Z","shell.execute_reply":"2024-04-06T01:59:17.272989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Part 1, 2, and 4 are common for all records\n\n- Part 5 is unique for all records\n\n- Part 3 has 100 distinct values, same as RecipeCode","metadata":{}},{"cell_type":"code","source":"cid['Part_3'].astype('int64').equals(train['RecipeCode'])","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:17.275636Z","iopub.execute_input":"2024-04-06T01:59:17.276022Z","iopub.status.idle":"2024-04-06T01:59:17.286540Z","shell.execute_reply.started":"2024-04-06T01:59:17.275986Z","shell.execute_reply":"2024-04-06T01:59:17.285372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As anticipated, Part 3 of CommentID and RecipeCode are the exact same values","metadata":{}},{"cell_type":"markdown","source":"**Hypothesis 2**\n\n> **Confirmed:** CommentID does not provide any extra information\n\n> **Conclusion:**\n> - Drop CommentID feature","metadata":{}},{"cell_type":"markdown","source":"### 3.2.3 Hypothesis 3: UserID and UserName are highly dependent features\n\nUserID and UserName have similar distinct value counts (10783 and 10628 respectively)","metadata":{}},{"cell_type":"code","source":"# Contingency Table for UsrID and UserName\nuid_uname_contingency_table = train.groupby('UserID')['UserName'].value_counts().unstack().replace(np.nan, 0)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:17.288348Z","iopub.execute_input":"2024-04-06T01:59:17.289111Z","iopub.status.idle":"2024-04-06T01:59:19.085202Z","shell.execute_reply.started":"2024-04-06T01:59:17.289063Z","shell.execute_reply":"2024-04-06T01:59:19.083902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perform $\\chi^2$ Test of Independence for UserID-UserName Contingency Table\n\n- $\\text{H}_0$: UserID and UserName are independent\n- $\\text{H}_{\\text{A}}$: UserID and UserName are dependent","metadata":{}},{"cell_type":"code","source":"from scipy.stats import chi2, chi2_contingency\n\n# Perform Chi-square Test of Independence\nchi2_comp, p_value, dof, expected_ct = chi2_contingency(uid_uname_contingency_table)\n\nalpha = 0.01\nchi2_tab = chi2.ppf(1 - alpha, dof)\n\nprint(f'Computed Chi-square:  {chi2_comp:.4f}')\nprint(f'Tabular Chi-square :  {chi2_tab:.4f}\\n')\n\nprint(f'p-Value            :  {p_value:.2f}')\nprint(f'Significance Level :  {alpha:.2f}')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:19.086876Z","iopub.execute_input":"2024-04-06T01:59:19.087587Z","iopub.status.idle":"2024-04-06T01:59:21.476477Z","shell.execute_reply.started":"2024-04-06T01:59:19.087540Z","shell.execute_reply":"2024-04-06T01:59:21.475131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reject or Fail to reject Null Hypothesis\nif chi2_comp > chi2_tab and p_value < alpha:\n    print('Reject H_0 and conclude UserID and UserName are dependent')\nelse:\n    print('Fail to reject H_0 and conclude UserID and UserName are independent')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:21.478082Z","iopub.execute_input":"2024-04-06T01:59:21.478543Z","iopub.status.idle":"2024-04-06T01:59:21.485322Z","shell.execute_reply.started":"2024-04-06T01:59:21.478497Z","shell.execute_reply":"2024-04-06T01:59:21.484288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Percentage Unique in UserID  : {train['UserID'].nunique() / train.shape[0] * 100:.2f}%\")\nprint(f\"Percentage Unique in UserName: {train['UserName'].nunique() / train.shape[0] * 100:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:21.486998Z","iopub.execute_input":"2024-04-06T01:59:21.487599Z","iopub.status.idle":"2024-04-06T01:59:21.508620Z","shell.execute_reply.started":"2024-04-06T01:59:21.487548Z","shell.execute_reply":"2024-04-06T01:59:21.507041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Hypothesis 3**\n\n> **Confirmed:** Reject $\\text{H}_0$ and conclude that UserName and UserID are highly dependent even at a significance level of 1%\n\n> **Conclusion:**\n> - Drop UserID and UserName because they are almost unique values (~80%)","metadata":{}},{"cell_type":"markdown","source":"<a name='Target'></a>\n## 3.3 Target Variable","metadata":{}},{"cell_type":"code","source":"# Frequency Distribution of Target Variable - 'Rating'\nabs_values = train['Rating'].value_counts().sort_index()\nrel_values = train['Rating'].value_counts(normalize=True).sort_index() * 100\nlbls = [f'{abs} ({rel:.1f}%)' for abs, rel in zip(abs_values, rel_values)]\n\nplt.figure(figsize=(10, 6))\nax = sns.countplot(data=train, x='Rating')\nplt.title('Frequency of Rating', y=1.02, fontsize=15)\nax.bar_label(container=ax.containers[0], labels=lbls)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:21.511719Z","iopub.execute_input":"2024-04-06T01:59:21.512225Z","iopub.status.idle":"2024-04-06T01:59:21.852499Z","shell.execute_reply.started":"2024-04-06T01:59:21.512178Z","shell.execute_reply":"2024-04-06T01:59:21.851351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"null_accuracy = train['Rating'].value_counts(normalize=True).max() * 100\nprint(f'Null Accuracy: {null_accuracy:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:21.853845Z","iopub.execute_input":"2024-04-06T01:59:21.854192Z","iopub.status.idle":"2024-04-06T01:59:21.863273Z","shell.execute_reply.started":"2024-04-06T01:59:21.854160Z","shell.execute_reply":"2024-04-06T01:59:21.861948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As evident from the values taken by the target variable, Rating, this Machine Learning model aims to predict an ordinal categorical variable. Hence, it can be considered as a _'Multi-class Classification'_ problem with 0-5 being the class labels.\n\nFrom the plot above, it is evident that the dataset is imbalanced. In fact, the null accuracy is of _76.06%_, which means even if the model predicts the most frequent rating everytime, it will achieve an accuarcy of _76.06%_.\n\nTherefore, 2 important aspects need to be considered:\n\n1. **Handling Imbalance**\n\n   Data Balancing techniques, like SMOTE, can be used; however, this might not be effective in this particular scenario. This is because of the Rating 0 that signifies the review has not been scored, meaning it may possess properties similar to other Ratings (1-5) and hence, may deviate the model from learning effectively. In fact, I tried using SMOTE and unfortunately, the model was tending to underfit.\n   \n   On the other hand, to handle data imbalance, while training the model, the dataset can be split in a stratified manner to maintain proportions for each class.\n   \n2. **Evaluation Metric**\n\n    Accuracy is testing metric for this competition; however, considering the imbalance, it might be better suited to evaluate the model on other metrics as well, including Precision, Recall, and F1 Score.","metadata":{}},{"cell_type":"markdown","source":"<a name='Missing-Duplicate'></a>\n## 3.4 Missing and Duplicate Values","metadata":{}},{"cell_type":"code","source":"# Check missing values in train\ntrain.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:21.864932Z","iopub.execute_input":"2024-04-06T01:59:21.865327Z","iopub.status.idle":"2024-04-06T01:59:21.888482Z","shell.execute_reply.started":"2024-04-06T01:59:21.865294Z","shell.execute_reply":"2024-04-06T01:59:21.886973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Missing values in Recipe_Review (train dataset): {train[\"Recipe_Review\"].isnull().sum()}')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:21.891228Z","iopub.execute_input":"2024-04-06T01:59:21.891973Z","iopub.status.idle":"2024-04-06T01:59:21.901751Z","shell.execute_reply.started":"2024-04-06T01:59:21.891930Z","shell.execute_reply":"2024-04-06T01:59:21.900294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check invalid values (not containing any alphanumeric character) in Recipe_Review in train\ndef no_alnum(x):\n    if isinstance(x, str):\n        return not any(char.isalnum() for char in x)\n    else:\n        return True\n\nprint(train[train['Recipe_Review'].apply(no_alnum)]['Recipe_Review'], end='\\n\\n')\n\nprint(f'Invalid values in Recipe_Review (train dataset): {train[train[\"Recipe_Review\"].apply(no_alnum)].shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:21.903662Z","iopub.execute_input":"2024-04-06T01:59:21.904174Z","iopub.status.idle":"2024-04-06T01:59:21.957381Z","shell.execute_reply.started":"2024-04-06T01:59:21.904126Z","shell.execute_reply":"2024-04-06T01:59:21.956014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check missing values in test\ntest.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:21.958978Z","iopub.execute_input":"2024-04-06T01:59:21.959341Z","iopub.status.idle":"2024-04-06T01:59:21.972731Z","shell.execute_reply.started":"2024-04-06T01:59:21.959307Z","shell.execute_reply":"2024-04-06T01:59:21.971127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking invalid values in Recipe_Review in test\nprint(test[test['Recipe_Review'].apply(no_alnum)]['Recipe_Review'], end='\\n\\n')\n\nprint(f'Invalid values in Recipe_Review (test dataset): {test[test[\"Recipe_Review\"].apply(no_alnum)].shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:21.983242Z","iopub.execute_input":"2024-04-06T01:59:21.983662Z","iopub.status.idle":"2024-04-06T01:59:22.007397Z","shell.execute_reply.started":"2024-04-06T01:59:21.983627Z","shell.execute_reply":"2024-04-06T01:59:22.006527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check duplicate records in train and test\ntrain.duplicated().any(), test.duplicated().any()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:22.008772Z","iopub.execute_input":"2024-04-06T01:59:22.009246Z","iopub.status.idle":"2024-04-06T01:59:22.048859Z","shell.execute_reply.started":"2024-04-06T01:59:22.009208Z","shell.execute_reply":"2024-04-06T01:59:22.047580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n\n- There are no duplicate records in the datasets.\n\n- There are 2 missing values in train dataset (Recipe_Review), while no missing values in test dataset.\n\n- Upon further inspection, it is found overall 10 and 4 invalid values (not containing any alphanumeric character) exist for Recipe_Review for train and test datasets respectively. \n\n<br>\n\nTo mitigate this issue, either (a) these values can be imputed with a constant value or (b) they can handled during the text vectorization step. In this notebook, option (b) has been opted.","metadata":{}},{"cell_type":"markdown","source":"<a name='Numeric'></a>\n## 3.5 Numerical Features","metadata":{}},{"cell_type":"code","source":"numerical_features = ['ID', 'RecipeNumber', 'UserReputation', 'ReplyCount', 'ThumbsUpCount', 'ThumbsDownCount', 'BestScore']","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:22.050240Z","iopub.execute_input":"2024-04-06T01:59:22.050652Z","iopub.status.idle":"2024-04-06T01:59:22.056467Z","shell.execute_reply.started":"2024-04-06T01:59:22.050615Z","shell.execute_reply":"2024-04-06T01:59:22.055176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"act = train[numerical_features]\n\n# Boxplot for Numeric Features\nplt.figure(figsize=(12, 6))\nsns.boxplot(act)\nplt.title('Boxplot for Numerical Features')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:22.057964Z","iopub.execute_input":"2024-04-06T01:59:22.058330Z","iopub.status.idle":"2024-04-06T01:59:22.474432Z","shell.execute_reply.started":"2024-04-06T01:59:22.058284Z","shell.execute_reply":"2024-04-06T01:59:22.472903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to find count and percentage of outliers using IQR method\ndef outliers(df):\n    Q1 = df.quantile(0.25)\n    Q3 = df.quantile(0.75)\n    IQR = Q3 - Q1\n    \n    temp = (df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))\n    return pd.DataFrame({\n        'Count': temp.sum(),\n        'Percentage': round(temp.mean() * 100, 2)\n    })","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:22.476514Z","iopub.execute_input":"2024-04-06T01:59:22.477091Z","iopub.status.idle":"2024-04-06T01:59:22.488011Z","shell.execute_reply.started":"2024-04-06T01:59:22.477031Z","shell.execute_reply":"2024-04-06T01:59:22.486000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Outlier Count for Original Numeric Features\nact_outlier = outliers(act)\nact_outlier","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:22.490132Z","iopub.execute_input":"2024-04-06T01:59:22.491549Z","iopub.status.idle":"2024-04-06T01:59:22.521108Z","shell.execute_reply.started":"2024-04-06T01:59:22.491503Z","shell.execute_reply":"2024-04-06T01:59:22.519686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bar plot of Outlier Count\nplt.figure(figsize=(15, 6))\nax = act_outlier['Count'].plot.bar(rot=0)\nlbls = [f'{act_outlier.loc[idx, \"Count\"]} ({act_outlier.loc[idx, \"Percentage\"]}%)' for idx in act_outlier.index]\nax.bar_label(container=ax.containers[0], labels=lbls)\nplt.ylabel('Count (Percentage %)')\nplt.title('Outliers per Numerical Feature')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:22.523148Z","iopub.execute_input":"2024-04-06T01:59:22.523709Z","iopub.status.idle":"2024-04-06T01:59:22.959388Z","shell.execute_reply.started":"2024-04-06T01:59:22.523675Z","shell.execute_reply":"2024-04-06T01:59:22.957870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the boxplot and the outlier count plot, it can be seen that there are a lot of outliers, especially for ThumbsUpCount, ThumbsDownCount, and BestScore features, based on the IQR-method.\n\nDropping / Imputing the respective records might cause a lot of information to be lost.\n\nTransforming these might seem a better solution to tackle outliers.","metadata":{}},{"cell_type":"markdown","source":"### Normalizer\n\nScales each sample individually to unit norm (in this case, L2-norm)","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import Normalizer\n\nnormalizer = Normalizer(norm='l2').set_output(transform='pandas')\nnorm = normalizer.fit_transform(act)\n\n# Boxplot for Normalized Numeric Features\nplt.figure(figsize=(12, 6))\nsns.boxplot(norm)\nplt.title('Boxplot for Numerical Features (Normalizer)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:22.961307Z","iopub.execute_input":"2024-04-06T01:59:22.961774Z","iopub.status.idle":"2024-04-06T01:59:23.507102Z","shell.execute_reply.started":"2024-04-06T01:59:22.961728Z","shell.execute_reply":"2024-04-06T01:59:23.505892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"norm_outlier = outliers(norm)\nnorm_outlier","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:23.509081Z","iopub.execute_input":"2024-04-06T01:59:23.509485Z","iopub.status.idle":"2024-04-06T01:59:23.534331Z","shell.execute_reply.started":"2024-04-06T01:59:23.509450Z","shell.execute_reply":"2024-04-06T01:59:23.533164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Quantile Transformer (Uniform)\n\nApplies non-linear transformation to map probability density function to a Uniform Distribution","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import QuantileTransformer\n\nqut = QuantileTransformer(n_quantiles=10, output_distribution='uniform', random_state=random_state).set_output(transform='pandas')\nqu = qut.fit_transform(act)\n\n# Boxplot for Uniform QuantileTransformed Numeric Features\nplt.figure(figsize=(12, 6))\nsns.boxplot(qu)\nplt.title('Boxplot for Numerical Features (QuantileTransformer - Uniform)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:23.535617Z","iopub.execute_input":"2024-04-06T01:59:23.536669Z","iopub.status.idle":"2024-04-06T01:59:23.896500Z","shell.execute_reply.started":"2024-04-06T01:59:23.536620Z","shell.execute_reply":"2024-04-06T01:59:23.895230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qu_outlier = outliers(qu)\nqu_outlier","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:23.898069Z","iopub.execute_input":"2024-04-06T01:59:23.898448Z","iopub.status.idle":"2024-04-06T01:59:23.920971Z","shell.execute_reply.started":"2024-04-06T01:59:23.898413Z","shell.execute_reply":"2024-04-06T01:59:23.919767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Quantile Transformer (Gaussian)\n\nApplies non-linear transformation to map probability density function to a Gaussian Distribution","metadata":{}},{"cell_type":"code","source":"qgt = QuantileTransformer(n_quantiles=5, output_distribution='normal', random_state=random_state).set_output(transform='pandas')\nqg = qgt.fit_transform(act)\n\n# Boxplot for Gaussian QuantileTransformed Numeric Features\nplt.figure(figsize=(12, 6))\nsns.boxplot(qg)\nplt.title('Boxplot for Numerical Features (QuantileTransformer - Gaussian)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:23.922834Z","iopub.execute_input":"2024-04-06T01:59:23.923566Z","iopub.status.idle":"2024-04-06T01:59:24.288603Z","shell.execute_reply.started":"2024-04-06T01:59:23.923519Z","shell.execute_reply":"2024-04-06T01:59:24.287447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qg_outlier = outliers(qg)\nqg_outlier","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:24.289971Z","iopub.execute_input":"2024-04-06T01:59:24.290327Z","iopub.status.idle":"2024-04-06T01:59:24.313484Z","shell.execute_reply.started":"2024-04-06T01:59:24.290277Z","shell.execute_reply":"2024-04-06T01:59:24.312201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PowerTransformer\n\n#### Box-Cox Transformer\n\nTransforms non-normal distribution by stabilising the variance\n\nIt is mathematically defined as\n$$\nx_{i}^{(\\lambda)} = \n\\begin{cases}\n\\frac{{x_{i}^\\lambda - 1}}{{\\lambda}}, & \\text{if } \\lambda \\neq 0 \\\\\n\\ln(x_i), & \\text{if } \\lambda = 0\n\\end{cases}\n$$\n\nwhere optimal value of $\\lambda$, parameter for stabilizing variance and minimizing skewness, is estimated by PowerTransformer through maximum likelihood","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\n\nbct = PowerTransformer(method='box-cox', standardize=True).set_output(transform='pandas')\nbc = bct.fit_transform(act + 0.01)    # Box-Cox Transformer works on strictly positive data\n\n# Boxplot for Box-Cox Numeric Features\nplt.figure(figsize=(12, 6))\nsns.boxplot(bc)\nplt.title('Boxplot for Numerical Features (Box-Cox)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:24.315457Z","iopub.execute_input":"2024-04-06T01:59:24.316240Z","iopub.status.idle":"2024-04-06T01:59:24.754652Z","shell.execute_reply.started":"2024-04-06T01:59:24.316202Z","shell.execute_reply":"2024-04-06T01:59:24.753083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bc_outlier = outliers(bc)\nbc_outlier","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:24.756981Z","iopub.execute_input":"2024-04-06T01:59:24.757453Z","iopub.status.idle":"2024-04-06T01:59:24.782179Z","shell.execute_reply.started":"2024-04-06T01:59:24.757407Z","shell.execute_reply":"2024-04-06T01:59:24.781140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Yeo-Johnson Transformer\n\nSimilar to Box-Cox Transformer, but works on negative values as well\n\nIt is mathematically defined as\n\n$$\nx_{i}^{(\\lambda)} = \n\\begin{cases}\n[(x_i + 1)^\\lambda - 1] / \\lambda, & \\text{if } \\lambda \\neq 0, x_i \\geq 0 \\\\\n\\ln(x_i + 1), & \\text{if } \\lambda = 0, x_i \\geq 0 \\\\\n-[(-x_i + 1)^{2 - \\lambda} - 1] / (2 - \\lambda), & \\text{if } \\lambda \\neq 2, x_i < 0 \\\\\n-\\ln(-x_i + 1), & \\text{if } \\lambda = 2, x_i < 0\n\\end{cases}\n$$\n\nwhere optimal value of $\\lambda$, parameter for stabilizing variance and minimizing skewness, is estimated by PowerTransformer through maximum likelihood","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\n\nyjt = PowerTransformer(method='yeo-johnson', standardize=True).set_output(transform='pandas')\nyj = yjt.fit_transform(act)\n\n# Boxplot for Yeo-Johnson Numeric Features\nplt.figure(figsize=(12, 6))\nsns.boxplot(yj)\nplt.title('Boxplot for Numerical Features (Yeo-Johnson)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:24.786620Z","iopub.execute_input":"2024-04-06T01:59:24.787033Z","iopub.status.idle":"2024-04-06T01:59:25.251291Z","shell.execute_reply.started":"2024-04-06T01:59:24.786998Z","shell.execute_reply":"2024-04-06T01:59:25.250043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yj_outlier = outliers(yj)\nyj_outlier","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:25.252927Z","iopub.execute_input":"2024-04-06T01:59:25.254051Z","iopub.status.idle":"2024-04-06T01:59:25.277585Z","shell.execute_reply.started":"2024-04-06T01:59:25.254000Z","shell.execute_reply":"2024-04-06T01:59:25.276585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comb = pd.concat([act_outlier, norm_outlier, qu_outlier, qg_outlier, bc_outlier, yj_outlier], axis=1)\ncomb.columns = [\n    'Actual Count',\n    'Actual Percentage',\n    'Normalizer Count',\n    'Normalizer Percentage',\n    'Uniform QuantileTransformer Count',\n    'Uniform QuantileTransformer Percentage',\n    'Gaussian QuantileTransformer Count',\n    'Gaussian QuantileTransformer Percentage',\n    'Box-Cox Count',\n    'Box-Cox Percentage',\n    'Yeo-Johnson Count',\n    'Yeo-Johnson Percentage'\n]\ncomb","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:25.278825Z","iopub.execute_input":"2024-04-06T01:59:25.279637Z","iopub.status.idle":"2024-04-06T01:59:25.304519Z","shell.execute_reply.started":"2024-04-06T01:59:25.279590Z","shell.execute_reply":"2024-04-06T01:59:25.303168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bar plot of Combined Outlier Count\nax = comb.iloc[:, 1::2].plot.bar(rot=0, figsize=(20, 8))\n[ax.bar_label(container=ax.containers[i], label_type='edge', padding=(30+10*np.sin(np.pi*(i+0.5))), arrowprops=dict(arrowstyle=\"-|>\", connectionstyle=\"angle,angleA=0,angleB=90,rad=10\")) for i in range(6)]\nplt.ylabel('Percentage (%)')\nplt.ylim((0, 27))\nplt.title('Outliers per Numerical Feature (Percentage)')\nplt.legend(['Actual', 'Normalizer', 'Quantile - Uniform', 'Quantile - Gaussian', 'Box-Cox', 'Yeo-Johnson'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:25.306047Z","iopub.execute_input":"2024-04-06T01:59:25.306507Z","iopub.status.idle":"2024-04-06T01:59:27.295220Z","shell.execute_reply.started":"2024-04-06T01:59:25.306469Z","shell.execute_reply":"2024-04-06T01:59:27.293878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above graph shows the impact of different transformers on reducing the number of outliers.\n\n- Normalizer seems most effective for columns ID, UserReputation (marginally effective), and BestScore\n\n- Uniform QuantileTransformer seems most effective for column ID\n\n- RecipeNumber had no outliers and can be scaled using MinMaxScaler, StandardScaler, or even a Transformer\n\n- None of the transformers were effective for ReplyCount, ThumbsUpCount, and ThumbsDownCount\n\n<br>\n\nFor UserReputation, ReplyCount, ThumbsUpCount, and ThumbsDownCount, it might be better to convert into bins or ordinal categories.","metadata":{}},{"cell_type":"markdown","source":"### Binning","metadata":{}},{"cell_type":"code","source":"train['UserReputation'].value_counts().sort_index()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:27.297143Z","iopub.execute_input":"2024-04-06T01:59:27.297965Z","iopub.status.idle":"2024-04-06T01:59:27.309561Z","shell.execute_reply.started":"2024-04-06T01:59:27.297915Z","shell.execute_reply":"2024-04-06T01:59:27.308104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to categorise UserReputation\ndef categorise_reputation(reputation):\n    if reputation <= 1:    # Low Reputation\n        return 0\n    elif reputation <= 50: # Medium Reputation\n        return 1\n    else:                  # High Reputation\n        return 2","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:27.311276Z","iopub.execute_input":"2024-04-06T01:59:27.311703Z","iopub.status.idle":"2024-04-06T01:59:27.319980Z","shell.execute_reply.started":"2024-04-06T01:59:27.311660Z","shell.execute_reply":"2024-04-06T01:59:27.319027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bin UserReputation in 3 categories: Low, Medium, and High\nbinned_reputation = pd.DataFrame(train['UserReputation'].apply(categorise_reputation))\n\nsns.countplot(binned_reputation, x='UserReputation')\nplt.title('Count of Categorised UserReputation')\n\noutliers(binned_reputation)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:27.322671Z","iopub.execute_input":"2024-04-06T01:59:27.323181Z","iopub.status.idle":"2024-04-06T01:59:27.648129Z","shell.execute_reply.started":"2024-04-06T01:59:27.323072Z","shell.execute_reply":"2024-04-06T01:59:27.646637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since majority of the records (_12708_ records) have UserReputation of 1, it seems unlikely that outliers can be removed through transformation. Thus, the feature has been converted as an ordinal data of 3 categories (Low: 0, Medium: 1, High: 2) to capture as much information as possible. However, given the skewness of this feature, it is unlikely that this feature will have high importance during prediction.","metadata":{}},{"cell_type":"code","source":"train['ReplyCount'].value_counts().sort_index()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:27.649550Z","iopub.execute_input":"2024-04-06T01:59:27.649939Z","iopub.status.idle":"2024-04-06T01:59:27.660576Z","shell.execute_reply.started":"2024-04-06T01:59:27.649905Z","shell.execute_reply":"2024-04-06T01:59:27.658957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similarly, for ReplyCount feature, the values are highly imbalanced with only 4 unique values. Thus, it might be better to convert it to a binary variable with value 1 if the review is replied to, else 0.","metadata":{}},{"cell_type":"markdown","source":"### Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Visualise distribution of ThumbsUpCount and ThumbsDownCount\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n\nsns.histplot(act[['ThumbsUpCount']], x='ThumbsUpCount', ax=ax[0], bins=8)\nax[0].set_title('Distribution of ThumbsUpCount')\nax[0].set_xlim(0, 130)\n\nsns.histplot(act[['ThumbsDownCount']], x='ThumbsDownCount', ax=ax[1], bins=12)\nax[1].set_title('Distribution of ThumbsDownCount')\nax[1].set_xlim(0, 130)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:27.661958Z","iopub.execute_input":"2024-04-06T01:59:27.662414Z","iopub.status.idle":"2024-04-06T01:59:28.262023Z","shell.execute_reply.started":"2024-04-06T01:59:27.662378Z","shell.execute_reply":"2024-04-06T01:59:28.260745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above histograms depict the imbalance and highly skewed ThumbsUpCount and ThumbsDownCount features. Binning as a binary variable seems appropriate solution.\n\nOn the other hand, engineering a new numerical feature using these might help capture certain underlying patterns. In this project, we will try to create a new feature that tries to capture the ratio of the 2 features, since such a feature is not captured using methods like PolynomialFeatures.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import FunctionTransformer\n\nact['UpDownRatio'] = (act['ThumbsUpCount'] + 1) / (act['ThumbsDownCount'] + 1)\n# 1 has been added to numerator and denominator to adjust for 0 values\n\n# taking log of the ratio to bring a more normal-like distribution\nudrt = FunctionTransformer(np.log10).set_output(transform='pandas')\nudr = udrt.fit_transform(act[['UpDownRatio']])\n\nfig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(12, 6))\n\nsns.histplot(act[['UpDownRatio']], x='UpDownRatio', bins=10, ax=ax[0])\nax[0].set_title('Distribution of UpDownRatio')\n\nsns.histplot(udr, x='UpDownRatio', bins=10, ax=ax[1])\nax[1].set_title('Distribution of log UpDownRatio')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:28.263483Z","iopub.execute_input":"2024-04-06T01:59:28.263992Z","iopub.status.idle":"2024-04-06T01:59:28.872148Z","shell.execute_reply.started":"2024-04-06T01:59:28.263951Z","shell.execute_reply":"2024-04-06T01:59:28.870771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apart from taking the ratio, its $\\log_{10}$ was taken to bring out a more normal distribution, which is generally much easier to interpret for models, like LogisticRegression, and is symmetric with a well-defined mean and standard deviation.","metadata":{}},{"cell_type":"markdown","source":"<a name='Date-Time'></a>\n## 3.6 Date-Time Feature","metadata":{}},{"cell_type":"code","source":"# Extract year, month, day of week, and hour from CreationTimestamp\ndt = pd.DataFrame()\ndt['CreationTimestamp'] = train['CreationTimestamp'].copy()\ndt['timestamp'] = pd.to_datetime(dt['CreationTimestamp'], unit='s')\ndt['year'] = dt['timestamp'].dt.year\ndt['month'] = dt['timestamp'].dt.month\ndt['dayofweek'] = dt['timestamp'].dt.dayofweek\ndt['hour'] = dt['timestamp'].dt.hour\ndt.drop(['CreationTimestamp', 'timestamp'], axis=1, inplace=True)\ndt","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:28.874141Z","iopub.execute_input":"2024-04-06T01:59:28.874608Z","iopub.status.idle":"2024-04-06T01:59:28.919603Z","shell.execute_reply.started":"2024-04-06T01:59:28.874563Z","shell.execute_reply":"2024-04-06T01:59:28.918388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(dt['year'].value_counts()).columns","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:28.921007Z","iopub.execute_input":"2024-04-06T01:59:28.921351Z","iopub.status.idle":"2024-04-06T01:59:28.933619Z","shell.execute_reply.started":"2024-04-06T01:59:28.921321Z","shell.execute_reply":"2024-04-06T01:59:28.932052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution across Years\nax = sns.countplot(dt[['year']], x='year')\nlbls = [f'{count} ({count / dt.shape[0] * 100:.2f}%)' for count in pd.DataFrame(dt['year'].value_counts())['count']]\nax.bar_label(container=ax.containers[0], labels=lbls)\nplt.title('Distribution across Years')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:28.935227Z","iopub.execute_input":"2024-04-06T01:59:28.935594Z","iopub.status.idle":"2024-04-06T01:59:29.224240Z","shell.execute_reply.started":"2024-04-06T01:59:28.935562Z","shell.execute_reply":"2024-04-06T01:59:29.223041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data is from years 2021-22 with majority (~97%) from 2021","metadata":{}},{"cell_type":"code","source":"# Distribution across Months\nplt.figure(figsize=(15, 6))\nax = sns.countplot(dt, x='month')\nlbls = [f'{count} ({count / dt.shape[0] * 100:.2f}%)' for count in pd.DataFrame(dt['month'].value_counts().sort_index())['count']]\nax.bar_label(container=ax.containers[0], labels=lbls)\nplt.title('Distribution across Months')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:29.225516Z","iopub.execute_input":"2024-04-06T01:59:29.225912Z","iopub.status.idle":"2024-04-06T01:59:29.669554Z","shell.execute_reply.started":"2024-04-06T01:59:29.225880Z","shell.execute_reply":"2024-04-06T01:59:29.668211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the Reviews are from June (6th  Month) (~95%)","metadata":{}},{"cell_type":"code","source":"# Distribution across Days of a Week\nplt.figure(figsize=(15, 6))\nax = sns.countplot(dt, x='dayofweek')\nlbls = [f'{count} ({count / dt.shape[0] * 100:.2f}%)' for count in pd.DataFrame(dt['dayofweek'].value_counts().sort_index())['count']]\nax.bar_label(container=ax.containers[0], labels=lbls)\nplt.title('Distribution across Days of a Week')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:29.671704Z","iopub.execute_input":"2024-04-06T01:59:29.672197Z","iopub.status.idle":"2024-04-06T01:59:30.045927Z","shell.execute_reply.started":"2024-04-06T01:59:29.672144Z","shell.execute_reply":"2024-04-06T01:59:30.044746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Majority of reviews are posted on Thursday (Day 4, Monday starts at 0) (~93%)","metadata":{}},{"cell_type":"code","source":"# Distribution across a Day\nplt.figure(figsize=(15, 6))\nax = sns.countplot(dt, x='hour')\nlbls = [f'{count}\\n({count / dt.shape[0] * 100:.1f}%)' for count in pd.DataFrame(dt['hour'].value_counts().sort_index())['count']]\nax.bar_label(container=ax.containers[0], labels=lbls)\nplt.title('Distribution across a Day')\nplt.ylim(0, 9500)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:30.047680Z","iopub.execute_input":"2024-04-06T01:59:30.048172Z","iopub.status.idle":"2024-04-06T01:59:30.623889Z","shell.execute_reply.started":"2024-04-06T01:59:30.048125Z","shell.execute_reply":"2024-04-06T01:59:30.622488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the Reviews are posted between 10-11 AM in the morning (~92% combined)\n\nOverall, it seems majority of Reviews are from Thursdays of June 2021 posted between 10-11 AM","metadata":{}},{"cell_type":"markdown","source":"In order to capture certain periodic aspects of the CreationTimestamp, a sine and cosine transformation can be employed to cyclically encode the features, which has an added benefit of scaling the values from -1 to 1.\n\nSine and Cosine Tranformation can be mathematically represented as\n$$\nx_s^* = \\sin\\left(\\frac{2 \\cdot \\pi \\cdot x}{\\text{period}}\\right)\\;,\\;\\; x_c^* = \\cos\\left(\\frac{2 \\cdot \\pi \\cdot x}{\\text{period}}\\right)\n$$\n\nwhere period will be 12 for month, 7 for day of week, and 24 for hour.","metadata":{}},{"cell_type":"markdown","source":"<a name='Corr-Matrix'></a>\n## 3.7 Correlation Matrices","metadata":{}},{"cell_type":"code","source":"# Correlation Matrix for Original Dataset\ntrain_corr = train.copy()\ntrain_corr.drop(['RecipeCode', 'RecipeName', 'CommentID', 'UserID', 'UserName', 'Recipe_Review', 'Rating'], axis=1, inplace=True)\ntrain_corr['Rating'] = train['Rating'].copy()\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(train_corr.corr().round(2), center=0, annot=True, cmap='RdYlGn')\nplt.title('Correlation Matrix (Original)')\n\nprint('== Correlation with Rating ==\\n')\nprint(abs(train_corr.corr()['Rating']).sort_values(ascending=False))\nprint()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T01:59:30.626001Z","iopub.execute_input":"2024-04-06T01:59:30.626975Z","iopub.status.idle":"2024-04-06T01:59:31.438385Z","shell.execute_reply.started":"2024-04-06T01:59:30.626927Z","shell.execute_reply":"2024-04-06T01:59:31.436903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation Matrix for Engineered Dataset\ntrain_corr = train.copy()\ntrain_corr.drop(['RecipeCode', 'RecipeName', 'CommentID', 'UserID', 'UserName', 'Recipe_Review', 'Rating'], axis=1, inplace=True)\ntrain_corr['UserReputation_Binned'] = train_corr['UserReputation'].apply(categorise_reputation)\ntrain_corr['ReplyCount_Binned'] = train_corr['ReplyCount'].apply(lambda x: 0 if x == 0 else 1)\ntrain_corr['ThumbsUpCount_Binned'] = train_corr['ThumbsUpCount'].apply(lambda x: 0 if x == 0 else 1)\ntrain_corr['ThumbsDownCount_Binned'] = train_corr['ThumbsDownCount'].apply(lambda x: 0 if x == 0 else 1)\ntrain_corr['UpDownRatio'] = (train_corr['ThumbsUpCount'] + 1) / (train_corr['ThumbsDownCount'] + 1)\ntrain_corr['timestamp'] = pd.to_datetime(train_corr['CreationTimestamp'], unit='s')\ntrain_corr['Year'] = train_corr['timestamp'].dt.year\ntrain_corr['Month_Sin'] = train_corr['timestamp'].dt.month.apply(lambda x: np.sin(x / 12 * 2 * np.pi))\ntrain_corr['Month_Cos'] = train_corr['timestamp'].dt.month.apply(lambda x: np.cos(x / 12 * 2 * np.pi))\ntrain_corr['DayOfWeek_Sin'] = train_corr['timestamp'].dt.dayofweek.apply(lambda x: np.sin(x / 7 * 2 * np.pi))\ntrain_corr['DayOfWeek_Cos'] = train_corr['timestamp'].dt.dayofweek.apply(lambda x: np.cos(x / 7 * 2 * np.pi))\ntrain_corr['Hour_Sin'] = train_corr['timestamp'].dt.hour.apply(lambda x: np.sin(x / 7 * 2 * np.pi))\ntrain_corr['Hour_Cos'] = train_corr['timestamp'].dt.hour.apply(lambda x: np.cos(x / 7 * 2 * np.pi))\ntrain_corr.drop(['UserReputation', 'ReplyCount', 'ThumbsUpCount',\n                 'ThumbsDownCount', 'CreationTimestamp','timestamp'], axis=1, inplace=True)\ntrain_corr['Rating'] = train['Rating'].copy()\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(train_corr.corr().round(2), center=0, annot=True, cmap='RdYlGn')\nplt.title('Correlation Matrix (Engineered)')\n\nprint('===== Correlation with Rating ====\\n')\nprint(abs(train_corr.corr()['Rating']).sort_values(ascending=False))\nprint()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T02:11:00.809970Z","iopub.execute_input":"2024-04-06T02:11:00.810438Z","iopub.status.idle":"2024-04-06T02:11:02.548233Z","shell.execute_reply.started":"2024-04-06T02:11:00.810400Z","shell.execute_reply":"2024-04-06T02:11:02.547070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Target variable 'Rating' seems to not have much correlation with any of the numerical or encoded features individually (highest being $\\sim\\left|-0.134\\right|$ correlation coefficient with the feature 'ThumbsDownCount_Binned'). This further signifies the importance of the text features for this modelling project.","metadata":{}},{"cell_type":"markdown","source":"<a name='EDA-Conclusion'></a>\n## 3.8 EDA Conclusions\n\nSummary of all insights useful for creating the preprocessor next\n\n- **Drop:** RecipeCode (Identifier); CommentID, UserID, UserName (Unique)\n- **Normalizer:** ID, RecipeNumber, BestScore\n- **Categorise:** UserReputation (0 if <= 1, 1 <= 50, 2 > 50), ReplyCount(0 if 0 else 1)\n- **Engineer:** $\\log$ of Ratio of ThumbsUpCount and ThumbsDownCount\n- **Extraction:** Year, Month, Day of Week and Hour from CreationTimestamp\n- **Vectorise:** RecipeName, Recipe_Review (handle missing values and punctuations)","metadata":{}},{"cell_type":"markdown","source":"<a name='Preprocessing'></a>\n# 4. Preprocessing","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import Normalizer, StandardScaler, FunctionTransformer\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:29:47.494825Z","iopub.execute_input":"2024-04-05T16:29:47.495203Z","iopub.status.idle":"2024-04-05T16:29:47.811398Z","shell.execute_reply.started":"2024-04-05T16:29:47.495170Z","shell.execute_reply":"2024-04-05T16:29:47.809793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_features = ['ID', 'BestScore', 'RecipeNumber', 'UserReputation', 'ReplyCount', 'ThumbsUpCount', 'ThumbsDownCount']\ndatetime_features = ['CreationTimestamp']\ntext_features = ['RecipeName', 'Recipe_Review']\ndrop_features = ['RecipeCode', 'CommentID', 'UserID', 'UserName']","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:29:47.812783Z","iopub.execute_input":"2024-04-05T16:29:47.813154Z","iopub.status.idle":"2024-04-05T16:29:47.819912Z","shell.execute_reply.started":"2024-04-05T16:29:47.813125Z","shell.execute_reply":"2024-04-05T16:29:47.818522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='Preprocessing-Numerical'></a>\n## 4.1 Numerical Features","metadata":{}},{"cell_type":"code","source":"# Helper to categorise UserReputation\ndef categorise_reputation(x):\n    categories = []\n    for reputation in x.values:\n        if reputation <= 1:\n            categories.append(0)\n        elif reputation <= 50:\n            categories.append(1)\n        else:\n            categories.append(2)\n    \n    return np.array(categories).reshape(-1, 1)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:29:47.821798Z","iopub.execute_input":"2024-04-05T16:29:47.822439Z","iopub.status.idle":"2024-04-05T16:29:47.831058Z","shell.execute_reply.started":"2024-04-05T16:29:47.822394Z","shell.execute_reply":"2024-04-05T16:29:47.829713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper to binarise ReplyCount\ndef replied_or_not(x):\n    replied = []\n    for reply in x.values:\n        if reply == 0:\n            replied.append(0)\n        else:\n            replied.append(1)\n\n    return np.array(replied).reshape(-1, 1)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:29:47.832765Z","iopub.execute_input":"2024-04-05T16:29:47.833232Z","iopub.status.idle":"2024-04-05T16:29:47.843322Z","shell.execute_reply.started":"2024-04-05T16:29:47.833187Z","shell.execute_reply":"2024-04-05T16:29:47.842032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper to engineer ThumbsUpCount to ThumbsDownCount Ratio\ndef engineer_ratio(x):\n    x = x.values\n    return np.log10((x[:, 0] + 1) / (x[:, 1] + 1)).reshape(-1, 1)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:29:47.844932Z","iopub.execute_input":"2024-04-05T16:29:47.845475Z","iopub.status.idle":"2024-04-05T16:29:47.858097Z","shell.execute_reply.started":"2024-04-05T16:29:47.845423Z","shell.execute_reply":"2024-04-05T16:29:47.856111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Numerical Preprocessor\n\nnumerical_ct = ColumnTransformer([\n#     ('normalizer', Normalizer(), ['ID', 'BestScore', 'RecipeNumber']),\n    ('normalizer', Normalizer(), ['ID', 'BestScore', 'RecipeNumber', 'ThumbsUpCount', 'ThumbsDownCount']),\n    ('encode_reputation', FunctionTransformer(categorise_reputation), ['UserReputation']),\n    ('encode_reply_count', FunctionTransformer(replied_or_not), ['ReplyCount']),\n    ('engineer_ratio', FunctionTransformer(engineer_ratio), ['ThumbsUpCount', 'ThumbsDownCount'])\n])\n\nnumerical_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('ct', numerical_ct)\n]) \nnumerical_transformer.set_output(transform='pandas')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:29:47.860191Z","iopub.execute_input":"2024-04-05T16:29:47.860578Z","iopub.status.idle":"2024-04-05T16:29:47.925955Z","shell.execute_reply.started":"2024-04-05T16:29:47.860547Z","shell.execute_reply":"2024-04-05T16:29:47.924722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='Preprocessing-DateTime'></a>\n## 4.2 DateTime Features","metadata":{}},{"cell_type":"code","source":"# Helpers to extract year, month, day of week and hour from CreationTimestamp\ndef year_extractor(x):\n    years = []\n    for timestamp in x:\n        year = pd.to_datetime(timestamp, unit='s').year\n        years.append(year)\n    return np.array(years)\n\ndef month_extractor(x):\n    months = []\n    for timestamp in x:\n        month = pd.to_datetime(timestamp, unit='s').month\n        months.append(month)\n    return np.array(months)\n\ndef day_of_week_extractor(x):\n    days = []\n    for timestamp in x:\n        day = pd.to_datetime(timestamp, unit='s').dayofweek\n        days.append(day)\n    return np.array(days)\n\ndef hour_extractor(x):\n    hours = []\n    for timestamp in x:\n        hour = pd.to_datetime(timestamp, unit='s').hour\n        hours.append(hour)\n    return np.array(hours)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:29:47.927596Z","iopub.execute_input":"2024-04-05T16:29:47.927924Z","iopub.status.idle":"2024-04-05T16:29:47.938301Z","shell.execute_reply.started":"2024-04-05T16:29:47.927896Z","shell.execute_reply":"2024-04-05T16:29:47.937002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sine and Cosine transformers to elaborate periodicity\ndef sin_transformer(period):\n    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n\ndef cos_transformer(period):\n    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:29:47.940384Z","iopub.execute_input":"2024-04-05T16:29:47.940817Z","iopub.status.idle":"2024-04-05T16:29:47.953416Z","shell.execute_reply.started":"2024-04-05T16:29:47.940774Z","shell.execute_reply":"2024-04-05T16:29:47.951833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DateTime Preprocessor\n\nyear_engineer = Pipeline([\n    ('extractor', FunctionTransformer(year_extractor)),\n    ('scaler', StandardScaler())\n])\n\nmonth_engineer = Pipeline([\n    ('extractor', FunctionTransformer(month_extractor)),\n    ('scaler', FeatureUnion([\n        ('sin', sin_transformer(period=12)),\n        ('cos', cos_transformer(period=12))\n    ]))\n])\n\nday_of_week_engineer = Pipeline([\n    ('extractor', FunctionTransformer(day_of_week_extractor)),\n    ('scaler', FeatureUnion([\n        ('sin', sin_transformer(period=7)),\n        ('cos', cos_transformer(period=7))\n    ]))\n])\n\nhour_engineer = Pipeline([\n    ('extractor', FunctionTransformer(hour_extractor)),\n    ('scaler', FeatureUnion([\n        ('sin', sin_transformer(period=24)),\n        ('cos', cos_transformer(period=24))\n    ]))\n])\n\ntimestamp_engineer = FeatureUnion([\n    ('year', year_engineer),\n    ('month', month_engineer),\n    ('day_of_week', day_of_week_engineer),\n    ('hour', hour_engineer)\n])\n\ntimestamp_transformer = Pipeline([\n  ('imputer', SimpleImputer(strategy='median')),\n  ('engineer', timestamp_engineer)\n])\ntimestamp_transformer","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:29:47.955460Z","iopub.execute_input":"2024-04-05T16:29:47.955935Z","iopub.status.idle":"2024-04-05T16:29:48.478487Z","shell.execute_reply.started":"2024-04-05T16:29:47.955892Z","shell.execute_reply":"2024-04-05T16:29:48.477331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='Preprocessing-Text'></a>\n## 4.3 Text Features - Latent Semantic Analysis (LSA)","metadata":{}},{"cell_type":"markdown","source":"**Latent Semantic Analysis (LSA)** is Natural Language Processing (NLP) techniques that analyses relationships between documents and terms. It is implemented in this notebook in a 3-step manner:\n\n1. **TF-IDF Vectorizer:** converts a collection of documents (corpus) to a matrix of Term Frequency-Inverse Document Frequency values computed as\n\n    $$\n    \\text{tf-idf}(t, d) = \\text{tf}(t, d) \\cdot \\text{idf}(t)\n    $$\n    \n    where $\\text{tf}(t, d)$ represents number of times term $t$ appears in document $d$ and $\\text{idf}(t)$ is defined as (with smoothing)\n    \n    $$\n    \\text{idf}(t) = \\log\\left( \\frac{1 + n}{1 + \\text{df}(t)}\\right) + 1\n    $$\n    \n    where $n$ is total number of documents and $\\text{df}(t)$ is the number of documents containing term $t$\n\n2. **Singluar Value Decomposition (SVD):** reduces the dimensionality of the vectorized matrix, allowing estimators to compute fast and remain stable\n\n3. **Normalizer:** SVD results are normalized; normalizer converts each document vector to unit norm, scaling the features and highlighting the most important terms in a document","metadata":{}},{"cell_type":"code","source":"# Custom preprocessor for the TfidfVectorizer\ndef preprocess_text(text):\n    text = text[0]\n\n    if isinstance(text, str):\n        # Convert HTML escaped characters - e.g. &#39; -> \"'\" | &amp; -> \"&\"\n        text = html.unescape(text)\n    \n        # Capture emojis used in a review and replace them with their Unicode name - e.g.  -> \"SMILING FACE WITH HEART-SHAPED EYES\"\n        emoji_pattern = r'[\\U0001F000-\\U0001F9FF]|[\\U0001F600-\\U0001F64F]|[\\U0001F300-\\U0001F5FF]|[\\U0001F680-\\U0001F6FF]|[\\U00002600-\\U000027BF]'\n\n        emojis = re.findall(emoji_pattern, text)\n        for emoji in emojis:\n            try:\n                unicode_name = unicodedata.name(emoji)\n                text = text.replace(emoji, ' ' + unicode_name + ' ')\n            except ValueError:\n                pass\n        \n        # Handle hyperlinks\n        text = re.sub(r'http\\S+', ' ', text)\n        \n        # Convert negative contractions - e.g. \"didn't\" -> \"did not\"\n        text = re.sub(f\"n't\", ' not', text)\n        \n        # Replace special characters with a space\n        text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n        # Convert emphasised words - e.g. 'So Gooooooood' -> 'So Good'\n        text = re.sub(r'([a-zA-Z])\\1+', r'\\1\\1', text)\n        # Trim multiple spaces to single space\n        text = re.sub(r'\\s+', ' ', text)\n        text = text.strip()\n        \n        # Handle empty strings\n        if text == '':\n            return 'missing'\n        else:\n            return text.lower()\n    \n    # Handles non-string text like np.nan\n    else:\n        return 'invalid'","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:29:48.480228Z","iopub.execute_input":"2024-04-05T16:29:48.480679Z","iopub.status.idle":"2024-04-05T16:29:48.492965Z","shell.execute_reply.started":"2024-04-05T16:29:48.480646Z","shell.execute_reply":"2024-04-05T16:29:48.492014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for No. of Components to capture in TruncatedSVD\nimport time\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nlsa_review = Pipeline([\n    ('imputer', SimpleImputer(strategy='constant', fill_value='')),\n    ('tfidf', TfidfVectorizer(preprocessor=preprocess_text, max_features=5_000)),\n    ('svd', TruncatedSVD(random_state=random_state))\n])\n\nlsa_recipe_name = Pipeline([\n    ('imputer', SimpleImputer(strategy='constant', fill_value='')),\n    ('tfidf', TfidfVectorizer(preprocessor=preprocess_text, max_features=5_000)),\n    ('svd', TruncatedSVD(random_state=random_state))\n])","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:29:48.495320Z","iopub.execute_input":"2024-04-05T16:29:48.496547Z","iopub.status.idle":"2024-04-05T16:29:48.522726Z","shell.execute_reply.started":"2024-04-05T16:29:48.496495Z","shell.execute_reply":"2024-04-05T16:29:48.521481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Running SVD for Vectorized Recipe_Review\nstart_time = time.time()\nprint('LSA for Recipe_Review: ', end='')\nlsa_review.set_params(svd__n_components=5000).fit_transform(train[['Recipe_Review']])\nend_time = time.time()\nprint(f'Completed in {end_time - start_time:.2f} s')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:29:48.527312Z","iopub.execute_input":"2024-04-05T16:29:48.528000Z","iopub.status.idle":"2024-04-05T16:32:53.830245Z","shell.execute_reply.started":"2024-04-05T16:29:48.527954Z","shell.execute_reply":"2024-04-05T16:32:53.828972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Running SVD for vectorized RecipeName\nstart_time = time.time()\nprint('LSA for RecipeName: ', end='')\nlsa_recipe_name.set_params(svd__n_components=185).fit_transform(train[['RecipeName']])\nend_time = time.time()\nprint(f'Completed in {end_time - start_time:.2f} s')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:32:53.831699Z","iopub.execute_input":"2024-04-05T16:32:53.832068Z","iopub.status.idle":"2024-04-05T16:32:55.743387Z","shell.execute_reply.started":"2024-04-05T16:32:53.832017Z","shell.execute_reply":"2024-04-05T16:32:55.742408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Computing the explained variance ratios for the SVDs\nreview_explained_variance_ratio = np.array([0] + list(lsa_review[-1].explained_variance_ratio_)).cumsum()\nrecipe_name_explained_variance_ratio = np.array([0] + list(lsa_recipe_name[-1].explained_variance_ratio_)).cumsum()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:32:55.745124Z","iopub.execute_input":"2024-04-05T16:32:55.745891Z","iopub.status.idle":"2024-04-05T16:32:55.754562Z","shell.execute_reply.started":"2024-04-05T16:32:55.745848Z","shell.execute_reply":"2024-04-05T16:32:55.753143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Cumulative Explained Variance against No. of Components for Recipe_Review\nplt.plot(review_explained_variance_ratio)\nplt.hlines(y=0.93, xmin=0, xmax=2000, linestyles='dashed', color='black')\nplt.vlines(x=2000, ymin=0, ymax=0.93, linestyles='dashed', color='black')\nplt.xlim([0, 5000])\nplt.ylim([0, 1])\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.title('Cumulative Explained Variance vs. No. of SVD Components (Recipe_Review)')\nplt.show()\n\nprint(f'Percentage of Explained Variance with 2000 SVD components: {review_explained_variance_ratio[2000] * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:32:55.756187Z","iopub.execute_input":"2024-04-05T16:32:55.756548Z","iopub.status.idle":"2024-04-05T16:32:56.086292Z","shell.execute_reply.started":"2024-04-05T16:32:55.756518Z","shell.execute_reply":"2024-04-05T16:32:56.084852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Cumulative Explained Variance against No. of Components for Recipe_Review\nplt.plot(recipe_name_explained_variance_ratio)\nplt.hlines(y=0.95, xmin=0, xmax=80, linestyles='dashed', color='black')\nplt.vlines(x=80, ymin=0, ymax=0.95, linestyles='dashed', color='black')\nplt.xlim([0, 185])\nplt.ylim([0, 1])\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('Cumulative Explained Variance vs. No. of SVD Components (RecipeName)')\nplt.show()\n\nprint(f'Percentage of Explained Variance with 80 SVD components: {recipe_name_explained_variance_ratio[80] * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:32:56.087909Z","iopub.execute_input":"2024-04-05T16:32:56.089172Z","iopub.status.idle":"2024-04-05T16:32:56.424286Z","shell.execute_reply.started":"2024-04-05T16:32:56.089123Z","shell.execute_reply":"2024-04-05T16:32:56.422827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As evident from the graphs above, about 2000 and 80 components of TruncatedSVD are able to capture ~93-95% of the explained variance.","metadata":{}},{"cell_type":"markdown","source":"Upon running GridSearchCV for TfidfVectorizer parameters using a baseline LogisticRegression model, the following best parameters were achieved.\n\n---\n\n**Recipe_Review Transformer**\n\nCode:\n```python\nlsa_review_pipe = Pipeline([\n    ('imputer', SimpleImputer(strategy='constant', fill_value='')),\n    ('tfidf', TfidfVectorizer(preprocessor=preprocess_text)),\n    ('svd', TruncatedSVD(n_components=2000, random_state=random_state)),\n    ('normalizer', Normalizer()),\n    ('classifier', LogisticRegression(max_iter=5000, random_state=random_state))\n])\n\nparam_grid = {\n    'tfidf__lowercase': [True, False],\n    'tfidf__stop_words': [None, 'english'],\n    'tfidf__max_features': [5000],\n    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]\n}\n\ncv = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=random_state)\n\ngrid_search = GridSearchCV(lsa_review_pipe, param_grid, cv=cv)\ngrid_search.fit(X_train[['Recipe_Review]], y_train)\n\ngrid_search.best_params_\n```\n\nOutput:\n```\n{'tfidf__lowercase': True,\n 'tfidf__max_features': 5000,\n 'tfidf__ngram_range': (1, 2),\n 'tfidf__stop_words': 'english'}\n```\n\n---\n\n**RecipeName Transformer**\n\nCode:\n```python\nlsa_recipe_name_pipe = Pipeline([\n    ('imputer', SimpleImputer(strategy='constant', fill_value='')),\n    ('tfidf', TfidfVectorizer(preprocessor=preprocess_text)),\n    ('svd', TruncatedSVD(n_components=80, random_state=random_state)),\n    ('normalizer', Normalizer()),\n    ('classifier', LogisticRegression(max_iter=5000, random_state=random_state))\n])\n\nparam_grid = {\n    'tfidf__lowercase': [True, False],\n    'tfidf__stop_words': [None, 'english'],\n    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]\n}\n\ncv = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=random_state)\n\ngrid_search = GridSearchCV(lsa_recipe_name_pipe, param_grid, cv=cv)\ngrid_search.fit(X_train[['RecipeName]], y_train)\n\ngrid_search.best_params_\n```\n\nOutput:\n```\n{'tfidf__lowercase': True,\n 'tfidf__ngram_range': (1, 1),\n 'tfidf__stop_words': None}\n```","metadata":{}},{"cell_type":"markdown","source":"Thus, now we can define the final text preprocessors","metadata":{}},{"cell_type":"code","source":"# Best Recipe_Review Transformer\nbest_params_review = {\n    'lowercase': True,\n    'max_features': 5000,\n    'ngram_range': (1, 2),\n    'stop_words': 'english'\n}\nrecipe_review_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='constant', fill_value='')),\n    ('tfidf', TfidfVectorizer(preprocessor=preprocess_text, **best_params_review)),\n    ('svd', TruncatedSVD(n_components=2000, random_state=random_state)),\n    ('normalizer', Normalizer())\n])\nrecipe_review_transformer","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:32:56.426443Z","iopub.execute_input":"2024-04-05T16:32:56.426932Z","iopub.status.idle":"2024-04-05T16:32:56.457188Z","shell.execute_reply.started":"2024-04-05T16:32:56.426886Z","shell.execute_reply":"2024-04-05T16:32:56.455793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Best RecipeName Transformer\nbest_params_name = {\n    'lowercase': True,\n    'ngram_range': (1, 1),\n    'stop_words': None\n}\nrecipe_name_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='constant', fill_value='')),\n    ('tfidf', TfidfVectorizer(preprocessor=preprocess_text, **best_params_name)),\n    ('svd', TruncatedSVD(n_components=80, random_state=random_state)),\n    ('normalizer', Normalizer(norm='l2'))\n])\nrecipe_name_transformer","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:32:56.458628Z","iopub.execute_input":"2024-04-05T16:32:56.459069Z","iopub.status.idle":"2024-04-05T16:32:56.485687Z","shell.execute_reply.started":"2024-04-05T16:32:56.459012Z","shell.execute_reply":"2024-04-05T16:32:56.484406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combined Text Transformer\ntext_transformer = ColumnTransformer([\n    ('review_lsa', recipe_review_transformer, ['Recipe_Review']),\n    ('name_lsa', recipe_name_transformer, ['RecipeName'])\n])\ntext_transformer","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:32:56.487376Z","iopub.execute_input":"2024-04-05T16:32:56.488154Z","iopub.status.idle":"2024-04-05T16:32:56.581383Z","shell.execute_reply.started":"2024-04-05T16:32:56.487998Z","shell.execute_reply":"2024-04-05T16:32:56.580175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='Preprocessing-Combined'></a>\n## 4.4 Combined Preprocessor","metadata":{}},{"cell_type":"code","source":"# Compilation of all Transformers\npreprocessor = ColumnTransformer([\n    ('numerical', numerical_transformer, numerical_features),\n    ('datetime', timestamp_transformer, datetime_features),\n    ('text', text_transformer, text_features),\n    ('drop', 'drop', drop_features)\n])\npreprocessor","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:32:56.583366Z","iopub.execute_input":"2024-04-05T16:32:56.584586Z","iopub.status.idle":"2024-04-05T16:32:57.786191Z","shell.execute_reply.started":"2024-04-05T16:32:56.584531Z","shell.execute_reply":"2024-04-05T16:32:57.784853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='Model-Training-Selection'></a>\n# 5. Model Training and Selection","metadata":{}},{"cell_type":"markdown","source":"<a name='Train-Validation'></a>\n## 5.1 Train-Validation Datasets\n\nSplitting train dataframe to training and validation sets in a stratified manner in the ratio 75:25 respectively","metadata":{}},{"cell_type":"code","source":"# Splitting in stratified manner\nfrom sklearn.model_selection import train_test_split\n\nX = train.drop(['Rating'], axis=1)\ny = train['Rating'].copy()\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, stratify=y, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:32:57.787747Z","iopub.execute_input":"2024-04-05T16:32:57.788647Z","iopub.status.idle":"2024-04-05T16:32:57.812763Z","shell.execute_reply.started":"2024-04-05T16:32:57.788612Z","shell.execute_reply":"2024-04-05T16:32:57.811182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sanity Check\nassert X_train.shape[0] == y_train.shape[0]\nassert X_val.shape[0] == y_val.shape[0]\nassert X_train.shape[1] == X_val.shape[1]\n\nX_train.shape, y_train.shape, X_val.shape, y_val.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:32:57.829184Z","iopub.execute_input":"2024-04-05T16:32:57.829588Z","iopub.status.idle":"2024-04-05T16:32:57.838614Z","shell.execute_reply.started":"2024-04-05T16:32:57.829556Z","shell.execute_reply":"2024-04-05T16:32:57.837347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing the sets\nX_train_preprocessed = preprocessor.fit_transform(X_train)\nX_val_preprocessed = preprocessor.transform(X_val)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:32:57.840836Z","iopub.execute_input":"2024-04-05T16:32:57.841363Z","iopub.status.idle":"2024-04-05T16:33:56.158628Z","shell.execute_reply.started":"2024-04-05T16:32:57.841318Z","shell.execute_reply":"2024-04-05T16:33:56.157636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Stratified CV for Cross Validation throughout the notebook\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\ncv = StratifiedShuffleSplit(n_splits=2, test_size=0.25, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:33:56.160519Z","iopub.execute_input":"2024-04-05T16:33:56.161354Z","iopub.status.idle":"2024-04-05T16:33:56.167495Z","shell.execute_reply.started":"2024-04-05T16:33:56.161311Z","shell.execute_reply":"2024-04-05T16:33:56.166229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='Helper'></a>\n## 5.2 Helper Functions","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_validate","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:33:56.169021Z","iopub.execute_input":"2024-04-05T16:33:56.169421Z","iopub.status.idle":"2024-04-05T16:33:56.181745Z","shell.execute_reply.started":"2024-04-05T16:33:56.169388Z","shell.execute_reply":"2024-04-05T16:33:56.180254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper function for validation against multiple metrics\ndef multi_metric_validation_curve(model, X, y, param_name, param_range, metrics, cv):\n    if isinstance(metrics, str):\n        metrics = [metrics]\n        \n    train_scores = {metric: [] for metric in metrics}\n    test_scores = {metric: [] for metric in metrics}\n\n    for param_val in param_range:\n        cv_results = cross_validate(model.set_params(**{param_name: param_val}), X_train_preprocessed, y_train,\n                                    scoring=metrics,\n                                    cv=cv, return_train_score=True, n_jobs=-1)\n        \n        for metric in metrics:\n            train_scores[metric].append(cv_results[f'train_{metric}'])\n            test_scores[metric].append(cv_results[f'test_{metric}'])\n\n    for metric in metrics:\n        train_scores[metric] = np.array(train_scores[metric])\n        test_scores[metric] = np.array(test_scores[metric])\n        \n    return train_scores, test_scores","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:33:56.183700Z","iopub.execute_input":"2024-04-05T16:33:56.184126Z","iopub.status.idle":"2024-04-05T16:33:56.197788Z","shell.execute_reply.started":"2024-04-05T16:33:56.184088Z","shell.execute_reply":"2024-04-05T16:33:56.196472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper function to plot Validation Curves\ndef plot_validation_curve(train_scores, test_scores, model_name, param_name, param_range, metric_name, logx=False, zoom_window=None):\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(11, 5))\n    \n    # Zoomed Out Plot\n    ax[0].set_title(f'Validation Curve for {model_name} for {param_name}')\n    ax[0].set_xlabel(param_name)\n    ax[0].set_ylabel(metric_name)\n    ax[0].set_ylim(0, 1)\n    \n    if logx:\n        ax[0].semilogx(param_range, train_scores_mean, label='Training', color='darkorange')\n    else:\n        ax[0].plot(param_range, train_scores_mean, label='Training', color='darkorange')\n    \n    ax[0].fill_between(\n        param_range,\n        train_scores_mean - train_scores_std,\n        train_scores_mean + train_scores_std,\n        alpha=0.2,\n        color=\"darkorange\"\n    )\n    \n    if logx:\n        ax[0].semilogx(param_range, test_scores_mean, label='Testing', color='blue')\n    else:\n        ax[0].plot(param_range, test_scores_mean, label='Testing', color='blue')\n    \n    ax[0].fill_between(\n        param_range,\n        test_scores_mean - test_scores_std,\n        test_scores_mean + test_scores_std,\n        alpha=0.2,\n        color=\"blue\"\n    )\n    \n    ax[0].legend(loc=\"best\")\n    \n    # Zoomed In Plot\n    ax[1].set_title(f'Validation Curve for {model_name} for {param_name} (Zoomed In)')\n    ax[1].set_xlabel(param_name)\n    ax[1].set_ylabel(metric_name)\n    if zoom_window:\n        ax[1].set_ylim(zoom_window)\n    else:\n        ax[1].set_ylim(min(train_scores.min(), test_scores.min()) - 0.05, max(train_scores.max(), test_scores.max()) + 0.05)\n    \n    if logx:\n        ax[1].semilogx(param_range, train_scores_mean, label='Training', color='darkorange')\n    else:\n        ax[1].plot(param_range, train_scores_mean, label='Training', color='darkorange')\n    \n    ax[1].fill_between(\n        param_range,\n        train_scores_mean - train_scores_std,\n        train_scores_mean + train_scores_std,\n        alpha=0.2,\n        color=\"darkorange\"\n    )\n    \n    if logx:\n        ax[1].semilogx(param_range, test_scores_mean, label='Testing', color='blue')\n    else:\n        ax[1].plot(param_range, test_scores_mean, label='Testing', color='blue')\n    \n    ax[1].fill_between(\n        param_range,\n        test_scores_mean - test_scores_std,\n        test_scores_mean + test_scores_std,\n        alpha=0.2,\n        color=\"blue\"\n    )\n    \n    ax[1].legend(loc=\"best\")\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:33:56.199517Z","iopub.execute_input":"2024-04-05T16:33:56.199926Z","iopub.status.idle":"2024-04-05T16:33:56.221903Z","shell.execute_reply.started":"2024-04-05T16:33:56.199891Z","shell.execute_reply":"2024-04-05T16:33:56.221024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\n# Dataframe to store metrics of tuned Models for comparison\nresults = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision Weighted', 'Recall Weighted', 'F1 Weighted', 'ROC AUC OVR Weighted'])","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:33:56.223601Z","iopub.execute_input":"2024-04-05T16:33:56.224637Z","iopub.status.idle":"2024-04-05T16:33:56.239993Z","shell.execute_reply.started":"2024-04-05T16:33:56.224591Z","shell.execute_reply":"2024-04-05T16:33:56.238810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper function to compute metrics for comparison and update in results dataframe\ndef compute_comparison_metrics(model_name, model):\n    y_train_pred = model.predict(X_train_preprocessed)\n    y_val_pred = model.predict(X_val_preprocessed)\n    \n    y_train_pred_proba = model.predict_proba(X_train_preprocessed)\n    y_val_pred_proba = model.predict_proba(X_val_preprocessed)\n    \n    accuracy = [[accuracy_score(y_train, y_train_pred), accuracy_score(y_val, y_val_pred)]]\n    \n    prf = [[metric(y_train, y_train_pred, average='weighted'), metric(y_val, y_val_pred, average='weighted')] \\\n           for metric in [precision_score, recall_score, f1_score]]\n    \n    roc_auc = [[roc_auc_score(y_train, y_train_pred_proba, average='weighted', multi_class='ovr'),\n                roc_auc_score(y_val, y_val_pred_proba, average='weighted', multi_class='ovr')]]\n    \n    results.loc[len(results.index)] = [model_name] + accuracy + prf + roc_auc","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:33:56.241762Z","iopub.execute_input":"2024-04-05T16:33:56.242244Z","iopub.status.idle":"2024-04-05T16:33:56.254057Z","shell.execute_reply.started":"2024-04-05T16:33:56.242198Z","shell.execute_reply":"2024-04-05T16:33:56.252851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='Model'></a>\n## 5.3 Models","metadata":{}},{"cell_type":"markdown","source":"<a name='Logistic-Regression'></a>\n### 5.3.1 Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import validation_curve","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:33:56.255495Z","iopub.execute_input":"2024-04-05T16:33:56.255853Z","iopub.status.idle":"2024-04-05T16:33:56.268560Z","shell.execute_reply.started":"2024-04-05T16:33:56.255821Z","shell.execute_reply":"2024-04-05T16:33:56.267478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tuning Logistic Regression for different regularization strengths\nlr = LogisticRegression(max_iter=5000, random_state=random_state, n_jobs=-1)\n\nC_range = np.logspace(-3, 1, 5)\n\ntrain_scores, test_scores = multi_metric_validation_curve(lr, X_train_preprocessed, y_train,\n                                                          param_name='C', param_range=C_range,\n                                                          metrics=['neg_log_loss', 'roc_auc_ovr_weighted'], cv=cv)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T05:02:41.627953Z","iopub.execute_input":"2024-04-05T05:02:41.629271Z","iopub.status.idle":"2024-04-05T05:04:03.559324Z","shell.execute_reply.started":"2024-04-05T05:02:41.629229Z","shell.execute_reply":"2024-04-05T05:04:03.557538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Validation Curve for Log Loss\nplot_validation_curve(-train_scores['neg_log_loss'], -test_scores['neg_log_loss'],\n                      'Logistic Regression', 'C', C_range,\n                      'Cross-Entropy Loss', logx=True, zoom_window=(0.6, 0.9))","metadata":{"execution":{"iopub.status.busy":"2024-04-05T05:04:03.562382Z","iopub.execute_input":"2024-04-05T05:04:03.562924Z","iopub.status.idle":"2024-04-05T05:04:04.652004Z","shell.execute_reply.started":"2024-04-05T05:04:03.562874Z","shell.execute_reply":"2024-04-05T05:04:04.650876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Validation Curve for ROC AUC\nplot_validation_curve(train_scores['roc_auc_ovr_weighted'], test_scores['roc_auc_ovr_weighted'],\n                      'Logistic Regression', 'C', C_range,\n                      'ROC AUC OVR Weighted', logx=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T05:04:04.653708Z","iopub.execute_input":"2024-04-05T05:04:04.654363Z","iopub.status.idle":"2024-04-05T05:04:05.943598Z","shell.execute_reply.started":"2024-04-05T05:04:04.654321Z","shell.execute_reply":"2024-04-05T05:04:05.942452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graphs above, C=1 seems most appropriate as it is the turning point for both Cross-Entropy Loss and ROC AUC.","metadata":{}},{"cell_type":"code","source":"# Tuned Logistic Regression\nlr = LogisticRegression(C=1, max_iter=5000, random_state=random_state, n_jobs=-1).fit(X_train_preprocessed, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:33:56.270001Z","iopub.execute_input":"2024-04-05T16:33:56.270407Z","iopub.status.idle":"2024-04-05T16:34:26.070768Z","shell.execute_reply.started":"2024-04-05T16:33:56.270372Z","shell.execute_reply":"2024-04-05T16:34:26.069240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute metrics for comparison for Logistic Regression\ncompute_comparison_metrics('Logistic Regression', lr)\nresults","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:34:26.073028Z","iopub.execute_input":"2024-04-05T16:34:26.073473Z","iopub.status.idle":"2024-04-05T16:34:26.428111Z","shell.execute_reply.started":"2024-04-05T16:34:26.073432Z","shell.execute_reply":"2024-04-05T16:34:26.426689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='KNN'></a>\n### 5.3.2 K-Nearest Neighbors","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:34:26.430169Z","iopub.execute_input":"2024-04-05T16:34:26.430579Z","iopub.status.idle":"2024-04-05T16:34:26.436492Z","shell.execute_reply.started":"2024-04-05T16:34:26.430543Z","shell.execute_reply":"2024-04-05T16:34:26.435222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hyperparameter Tuning KNN\n\n**Code:**\n```python\nknn = KNeighborsClassifier(n_jobs=-1)\n\nparam_grid = {'n_neighbors': range(5, 110, 10), 'metric': ['euclidean', 'manhattan', 'cosine']}\n\ngrid_search = GridSearchCV(knn, scoring='roc_auc_ovr_weighted', param_grid=param_grid, cv=cv)\ngrid_search.fit(X_train_preprocessed, y_train)\n\nbest_params = grid_search.best_params_\nbest_params\n```\n\n**Output:**\n```\n{'metric': 'cosine', 'n_neighbors': 25}\n```","metadata":{}},{"cell_type":"markdown","source":"In certain text analysis models, Cosine Similarity is used to score how close two texts are that post-vectortization, just like using TfidfVectorizer for LSA in this notebook. Hence, Cosine Distance, which is defined as\n$$\n\\text{Cosine Distance}(x_1, x_2) = 1 - \\text{Cosine Similarity}(x_1, x_2) = 1 - \\frac{x_1x_2^\\top}{\\|x_1\\|\\|x_2\\|}\n$$\nis also being used to train and validate K-Nearest Neighbors.","metadata":{}},{"cell_type":"code","source":"# Tuned KNN\nbest_params = {'metric': 'cosine', 'n_neighbors': 25}\nknn = KNeighborsClassifier(**best_params, n_jobs=-1).fit(X_train_preprocessed, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:34:26.438655Z","iopub.execute_input":"2024-04-05T16:34:26.439067Z","iopub.status.idle":"2024-04-05T16:34:26.474125Z","shell.execute_reply.started":"2024-04-05T16:34:26.439006Z","shell.execute_reply":"2024-04-05T16:34:26.472802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute metrics for comparison for KNN\ncompute_comparison_metrics('K Nearest Neighbors', knn)\nresults","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:34:26.476205Z","iopub.execute_input":"2024-04-05T16:34:26.476592Z","iopub.status.idle":"2024-04-05T16:34:54.753685Z","shell.execute_reply.started":"2024-04-05T16:34:26.476558Z","shell.execute_reply":"2024-04-05T16:34:54.752792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='SVC'></a>\n### 5.3.3 Support Vector Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:34:54.755019Z","iopub.execute_input":"2024-04-05T16:34:54.755972Z","iopub.status.idle":"2024-04-05T16:34:54.761813Z","shell.execute_reply.started":"2024-04-05T16:34:54.755935Z","shell.execute_reply":"2024-04-05T16:34:54.760262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hyperparameter Tuning SVC\n\n**Code:**\n```python\nsvc = SVC(random_state=random_state)\nparam_grid = {\n    'C': np.logspace(-3, 1, 5),\n    'kernel': ['linear', 'rbf', 'poly', 'sigmoid']\n}\n\ngrid_search = GridSearchCV(svc, param_grid=param_grid, cv=cv)\ngrid_search.fit(X_train_preprocessed, y_train)\n\nbest_params = grid_search.best_params_\n```\n\n**Output:**\n```\n{'C': 10, 'kernel': 'rbf'}\n```","metadata":{}},{"cell_type":"markdown","source":"Post running the GridSearchCV, RBF kernel provided the best overall scores across different values of C; however, C=10 came out as best param for C, which is a corner case of ```np.logspace(-3, 1, 5)```. Thus, a validation plot for a larger range of C is plot next, choosing kernel as RBF.","metadata":{}},{"cell_type":"markdown","source":"**Code:**\n```python\n# Tuning SVC for various regularization strengths given Radial-Basis Function kernel\nsvc = SVC(kernel='rbf', probability=True, random_state=random_state)\n\nC_range = np.logspace(-1, 2, 4)\n\ntrain_scores, test_scores = multi_metric_validation_curve(svc, X_train_preprocessed, y_train,\n                                                          param_name='C', param_range=C_range,\n                                                          metrics=['f1_weighted', 'roc_auc_ovr_weighted'],\n                                                          cv=cv)\n```\n\n---\n\n**Code:**\n```python\n# Plot Validation Curve for F1 Score\nplot_validation_curve(train_scores['f1_weighted'], test_scores['f1_weighted'],\n                      'RBF SVC', 'C', C_range,\n                      'Weighted F1', logx=True)\n```\n\n**Output:**\n\n<img src='https://drive.google.com/thumbnail?id=1wiCLzjOb1i1lrlx2rcm85Ys7Msyjbwu3&sz=w1000' width='1000' alt='SVC Validation Curve for F1 Score'>\n\n[Click to Open Image](https://drive.google.com/file/d/1wiCLzjOb1i1lrlx2rcm85Ys7Msyjbwu3/view?usp=drive_link)\n\n---\n\n**Code:**\n```python\n# Plot Validation Curve for ROC AUC\nplot_validation_curve(train_scores['roc_auc_ovr_weighted'], test_scores['roc_auc_ovr_weighted'],\n                      'RBF SVC', 'C', C_range,\n                      'ROC AUC OVR Weighted', logx=True)\n```\n\n**Output:**\n\n<img src='https://drive.google.com/thumbnail?id=1H5vqyj8l-WJ7famvFNjnnj3g0Az4gj8f&sz=w1000' width='1000' alt='SVC Validation Curve for ROC AUC against C'>\n\n[Click to Open Image](https://drive.google.com/file/d/1H5vqyj8l-WJ7famvFNjnnj3g0Az4gj8f/view?usp=drive_link)","metadata":{}},{"cell_type":"markdown","source":"Given the shape of graphs and likeliness of overfitting, ideal C should lie in the range [1, 10]. Thus, even though C=10 seems best, C=5 is considered for building the model.","metadata":{}},{"cell_type":"code","source":"# Tuned SVC\nbest_params = {'C':5, 'kernel': 'rbf'}\nsvc = SVC(**best_params, probability=True, random_state=random_state).fit(X_train_preprocessed, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:34:54.763854Z","iopub.execute_input":"2024-04-05T16:34:54.764705Z","iopub.status.idle":"2024-04-05T16:52:52.032792Z","shell.execute_reply.started":"2024-04-05T16:34:54.764656Z","shell.execute_reply":"2024-04-05T16:52:52.031631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute metrics for comparison for SVC\ncompute_comparison_metrics('SVC', svc)\nresults","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:52:52.034383Z","iopub.execute_input":"2024-04-05T16:52:52.034832Z","iopub.status.idle":"2024-04-05T17:01:12.368300Z","shell.execute_reply.started":"2024-04-05T16:52:52.034791Z","shell.execute_reply":"2024-04-05T17:01:12.367168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='DT'></a>\n### 5.3.4 Decision Tree","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier, plot_tree","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:01:12.369957Z","iopub.execute_input":"2024-04-05T17:01:12.370599Z","iopub.status.idle":"2024-04-05T17:01:12.413755Z","shell.execute_reply.started":"2024-04-05T17:01:12.370565Z","shell.execute_reply":"2024-04-05T17:01:12.412230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training a fully grown Decision Tree\ndt = DecisionTreeClassifier(random_state=random_state).fit(X_train_preprocessed, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T13:04:19.295157Z","iopub.execute_input":"2024-04-05T13:04:19.295574Z","iopub.status.idle":"2024-04-05T13:07:03.068091Z","shell.execute_reply.started":"2024-04-05T13:04:19.295540Z","shell.execute_reply":"2024-04-05T13:07:03.066684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking dimensions of a fully grown tree\ndt.get_depth(), dt.get_n_leaves()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T05:45:46.741311Z","iopub.execute_input":"2024-04-05T05:45:46.741803Z","iopub.status.idle":"2024-04-05T05:45:46.750567Z","shell.execute_reply.started":"2024-04-05T05:45:46.741758Z","shell.execute_reply":"2024-04-05T05:45:46.749460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the Decision Tree\nplt.figure(figsize=(5, 8))\nplot_tree(dt)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T05:45:46.752238Z","iopub.execute_input":"2024-04-05T05:45:46.752554Z","iopub.status.idle":"2024-04-05T05:46:54.351097Z","shell.execute_reply.started":"2024-04-05T05:45:46.752526Z","shell.execute_reply":"2024-04-05T05:46:54.349806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A Decison Tree with long 'branches', depth of 134, and 1388 leaf nodes for just 6 classes seems excessive and very prone to overfitting as it may try to fit the noise. Thus, the tree must be pruned. In this notebook, the Decision Tree will be **post-pruned**.","metadata":{}},{"cell_type":"markdown","source":"#### **Post-pruning the Decision Tree with Minimal Cost-Complexity Pruning**\n\nPruning is done to prevent the tree from overfitting. The Cost-Complexity measure $R_{\\alpha}(T)$ for a given Tree $T$ and a complexity parameter $\\alpha \\geq 0$ is\n\n$$\nR_{\\alpha}(T) = R(T) + \\alpha|\\tilde{T}|\n$$\n\nwhere $|\\tilde{T}|$ is number of terminal nodes and $R(T)$ is the total weighted sample impurity of the terminal nodes. Minimal Cost-Complexity Pruning tries to find the subtree of $T$ that minimizes $R_{\\alpha}(T)$. ","metadata":{}},{"cell_type":"code","source":"# Capturing Impurities for various CCP alphas\npath = dt.cost_complexity_pruning_path(X_train_preprocessed, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities","metadata":{"execution":{"iopub.status.busy":"2024-04-05T05:46:54.353088Z","iopub.execute_input":"2024-04-05T05:46:54.353398Z","iopub.status.idle":"2024-04-05T05:49:37.581494Z","shell.execute_reply.started":"2024-04-05T05:46:54.353370Z","shell.execute_reply":"2024-04-05T05:49:37.580047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the effect of CCP alpha on the Impurity\nfig, ax = plt.subplots()\nax.plot(ccp_alphas[:-1], impurities[:-1], drawstyle=\"steps-post\")\nax.set_xlabel(\"Effective Alpha\")\nax.set_ylabel(\"Total Impurity of Leaves\")\nax.vlines(x=0.0008, ymin=0, ymax=0.36, color='black', linestyles='dashed')\nax.hlines(y=0.36, xmin=0, xmax=0.0008, color='black', linestyles='dashed')\nax.set_xlim(0, 0.0055)\nax.set_ylim(0, 0.4)\nax.set_title(\"Total Impurity vs Effective Alpha for Training Set\")\nplt.show()\n\nprint('\\nElbow at Alpha~=0.0008')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T05:49:37.583623Z","iopub.execute_input":"2024-04-05T05:49:37.584126Z","iopub.status.idle":"2024-04-05T05:49:37.899566Z","shell.execute_reply.started":"2024-04-05T05:49:37.584086Z","shell.execute_reply":"2024-04-05T05:49:37.898390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Code:**\n```python\n# Training Decision Tree for a few chosen CCP alphas\nalphas = [0, 0.0005, 0.0008, 0.001, 0.005]\n\ndts = []\nfor ccp_alpha in alphas:\n    dt = DecisionTreeClassifier(random_state=random_state, ccp_alpha=ccp_alpha).fit(X_train_preprocessed, y_train)\n    dts.append(dt)\n```\n\n---\n\n**Code:**\n```python\n# Plotting effect of CCP alpha on Decision Tree Depth and Number of Nodes\nnode_counts = [dt.tree_.node_count for dt in dts]\ndepth = [dt.tree_.max_depth for dt in dts]\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n\nax[0].plot(alphas, node_counts, marker=\"o\")\nax[0].set_xlabel(\"Alpha\")\nax[0].set_ylabel(\"Number of Nodes\")\nax[0].set_title(\"Number of Nodes vs Alpha\")\n\nax[1].plot(alphas, depth, marker=\"o\")\nax[1].set_xlabel(\"Alpha\")\nax[1].set_ylabel(\"Depth of Tree\")\nax[1].set_title(\"Depth vs Alpha\")\n\nplt.show()\n```\n\n**Output:**\n\n<img src='https://drive.google.com/thumbnail?id=1t02qH6wgxk16Dh8m1e11AJpAdqZiDpbv&sz=w1000' width='1000' alt='Effect of ccp_alpha on number of nodes and depth of DT'>\n\n[Click to Open Image](https://drive.google.com/file/d/1t02qH6wgxk16Dh8m1e11AJpAdqZiDpbv/view?usp=drive_link)","metadata":{}},{"cell_type":"markdown","source":"The plot above shows how ccp_alpha itself affects the number of nodes and depth of a Decision Tree and thus, might be an effective way to prune it.","metadata":{}},{"cell_type":"markdown","source":"**Code:**\n```python\n# Tuning Decision Tree for various CCP alphas\ndt = DecisionTreeClassifier(random_state=random_state)\n\nccp_alpha_range = [0, 0.0005, 0.0008, 0.001, 0.005]\n\ntrain_scores, test_scores = multi_metric_validation_curve(dt, X_train_preprocessed, y_train,\n                                                          param_name='ccp_alpha', param_range=ccp_alpha_range,\n                                                          metrics=['neg_log_loss', 'f1_weighted', 'roc_auc_ovr_weighted'],\n                                                          cv=cv)\n```\n\n---\n\n**Code:**\n```python\n# Plot Validation Curve for Log Loss\nplot_validation_curve(-train_scores['neg_log_loss'], -test_scores['neg_log_loss'],\n                      'Decision Tree', 'CCP Alpha', ccp_alpha_range,\n                      'Cross-Entropy Loss')\n```\n\n**Output:**\n\n<img src='https://drive.google.com/thumbnail?id=1uPUSdQQu6zO60dB8UobTqpnEN7eFKioh&sz=w1000' width='1000' alt='DT Validation Curve for LogLoss against ccp_alpha'>\n\n[Click to Open Image](https://drive.google.com/file/d/1uPUSdQQu6zO60dB8UobTqpnEN7eFKioh/view?usp=drive_link)\n\n---\n\n**Code:**\n\n```python\n# Plot Validation Curve for F1 Score\nplot_validation_curve(train_scores['f1_weighted'], test_scores['f1_weighted'],\n                      'Decision Tree', 'CCP Alpha', ccp_alpha_range,\n                      'F1 Weighted')\n```\n\n**Output:**\n\n<img src='https://drive.google.com/thumbnail?id=1NetNG86Nw8zSE7zoWAHycAF2dtSgS8Eg&sz=w1000' width='1000' alt='DT Validation Curve for F1 against ccp_alpha'>\n\n[Click to Open Image](https://drive.google.com/file/d/1NetNG86Nw8zSE7zoWAHycAF2dtSgS8Eg/view?usp=drive_link)\n\n___\n\n**Code:**\n```python\n# Plot Validation Curve for ROC AUC\nplot_validation_curve(train_scores['roc_auc_ovr_weighted'], test_scores['roc_auc_ovr_weighted'],\n                      'Decision Tree', 'CCP Alpha', ccp_alpha_range,\n                      'ROC AUC OVR Weighted')\n```\n\n**Output:**\n\n<img src='https://drive.google.com/thumbnail?id=10t0K4WXwY3AvZOLLA4UAgAabKvt2_t2f&sz=w1000' width='1000' alt='DT Validation Curve for ROC AUC against ccp_alpha'>\n\n[Click to Open Image](https://drive.google.com/file/d/10t0K4WXwY3AvZOLLA4UAgAabKvt2_t2f/view?usp=drive_link)","metadata":{}},{"cell_type":"markdown","source":"From the plots above, ccp_alpha=0.001 seems appropriate for relatively good in terms of Weighted F1 and ROC AUC; furthermore, Cross-Entropy Loss approximately stagnates post ccp_alpha=0.001.","metadata":{}},{"cell_type":"code","source":"# Tuned Decision Tree\ndt = DecisionTreeClassifier(ccp_alpha=0.001, random_state=random_state).fit(X_train_preprocessed, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:01:12.418723Z","iopub.execute_input":"2024-04-05T17:01:12.419153Z","iopub.status.idle":"2024-04-05T17:03:55.298540Z","shell.execute_reply.started":"2024-04-05T17:01:12.419117Z","shell.execute_reply":"2024-04-05T17:03:55.296984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute metrics for comparison for Decision Tree\ncompute_comparison_metrics('Decision Tree', dt)\nresults","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:03:55.300405Z","iopub.execute_input":"2024-04-05T17:03:55.300905Z","iopub.status.idle":"2024-04-05T17:03:55.547200Z","shell.execute_reply.started":"2024-04-05T17:03:55.300860Z","shell.execute_reply":"2024-04-05T17:03:55.545525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='Random-Forest'></a>\n### 5.3.5 Random Forest - Bagging","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:03:55.548903Z","iopub.execute_input":"2024-04-05T17:03:55.549394Z","iopub.status.idle":"2024-04-05T17:03:55.825730Z","shell.execute_reply.started":"2024-04-05T17:03:55.549353Z","shell.execute_reply":"2024-04-05T17:03:55.824036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since ccp_alpha=0.001 gives best Decision Tree, the same value is considered to train the Random Forest. Also, boostrapping is chosen with a max_features of 'sqrt' of total number of features.","metadata":{}},{"cell_type":"code","source":"# Tuning Random Forest for number of estimators\nrf = RandomForestClassifier(bootstrap=True, max_features='sqrt', ccp_alpha=0.001, n_jobs=-1, random_state=random_state)\n\nn_estimators_range = [5, 10, 20, 50, 100]\n\ntrain_scores, test_scores = multi_metric_validation_curve(rf, X_train_preprocessed, y_train,\n                                                          param_name='n_estimators', param_range=n_estimators_range,\n                                                          metrics=['neg_log_loss', 'roc_auc_ovr_weighted'],\n                                                          cv=cv)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:15:41.178802Z","iopub.execute_input":"2024-04-05T06:15:41.180542Z","iopub.status.idle":"2024-04-05T06:18:29.584274Z","shell.execute_reply.started":"2024-04-05T06:15:41.180482Z","shell.execute_reply":"2024-04-05T06:18:29.582788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Validation Curve for Log Loss\nplot_validation_curve(-train_scores['neg_log_loss'], -test_scores['neg_log_loss'],\n                      'RF', 'No. of Trees', n_estimators_range,\n                      'Cross-Entropy Loss')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:18:59.470583Z","iopub.execute_input":"2024-04-05T06:18:59.471122Z","iopub.status.idle":"2024-04-05T06:18:59.976363Z","shell.execute_reply.started":"2024-04-05T06:18:59.471083Z","shell.execute_reply":"2024-04-05T06:18:59.975238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Validation Curve for ROC AUC\nplot_validation_curve(train_scores['roc_auc_ovr_weighted'], test_scores['roc_auc_ovr_weighted'],\n                      'RF', 'No. of Trees', n_estimators_range,\n                      'ROC AUC OVR Weighted')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T06:19:02.034988Z","iopub.execute_input":"2024-04-05T06:19:02.036016Z","iopub.status.idle":"2024-04-05T06:19:02.529540Z","shell.execute_reply.started":"2024-04-05T06:19:02.035972Z","shell.execute_reply":"2024-04-05T06:19:02.528384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cross-Entropy Loss stagnates post-20 estimators, yet ROC AUC is maximum for 100 estimators. Thus, a value in between 20 and 100 (n_estimators=50) is considered.","metadata":{}},{"cell_type":"code","source":"# Tuned Random Forest\nrf = RandomForestClassifier(n_estimators=50, bootstrap=True, max_features='sqrt',\n                            ccp_alpha=0.001, n_jobs=-1, random_state=random_state).fit(X_train_preprocessed, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:03:55.827358Z","iopub.execute_input":"2024-04-05T17:03:55.827741Z","iopub.status.idle":"2024-04-05T17:04:28.400447Z","shell.execute_reply.started":"2024-04-05T17:03:55.827710Z","shell.execute_reply":"2024-04-05T17:04:28.398981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute metrics for comparison for Random Forest\ncompute_comparison_metrics('Random Forest', rf)\nresults","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:04:28.401679Z","iopub.execute_input":"2024-04-05T17:04:28.402056Z","iopub.status.idle":"2024-04-05T17:04:28.818662Z","shell.execute_reply.started":"2024-04-05T17:04:28.402011Z","shell.execute_reply":"2024-04-05T17:04:28.817508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='XGBoost'></a>\n### 5.3.6 XGBoostClassifier - Boosting","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:04:28.819910Z","iopub.execute_input":"2024-04-05T17:04:28.820264Z","iopub.status.idle":"2024-04-05T17:04:29.057527Z","shell.execute_reply.started":"2024-04-05T17:04:28.820234Z","shell.execute_reply":"2024-04-05T17:04:29.056388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Given the importance of parameters for a Gradient Boosting Algorithm, the XGBClassifier will first be tuned for learning_rate, followed by the n_estimators; both of these parameters will be further used to tune tree-specific parameters, such as max_depth.","metadata":{}},{"cell_type":"markdown","source":"**Code:**\n```python\n# Tuning XGBClassifier for learning_rate\nxgb = XGBClassifier(objective='multi:softmax', n_jobs=-1, random_state=random_state)\nlearning_rate_range = [0.005, 0.01, 0.05, 0.1]\n\ntrain_scores, test_scores = multi_metric_validation_curve(xgb, X_train_preprocessed, y_train,\n                                                          param_name='learning_rate', param_range=learning_rate_range,\n                                                          metrics=['neg_log_loss', 'roc_auc_ovr_weighted'],\n                                                          cv=cv)\n```\n\n---\n\n**Code:**\n```python\n# Plot validation curve for Cross-Entropy Loss\nplot_validation_curve(-train_scores['neg_log_loss'], -test_scores['neg_log_loss'],\n                      'XGBoost', 'learning_rate', learning_rate_range,\n                      'Cross-Entropy Loss')\n```\n\n**Output:**\n\n<img src='https://drive.google.com/thumbnail?id=103cMXUlL-Ud6euAbVfHyr8ztjfMfe6Z9&sz=w1000' width='1000' alt='XGB Validation Curve for LogLoss against learning_rate'>\n\n[Click to Open Image](https://drive.google.com/file/d/103cMXUlL-Ud6euAbVfHyr8ztjfMfe6Z9/view?usp=drive_link)\n\n---\n\n**Code:**\n```python\n# Plot validation curve for ROC AUC\nplot_validation_curve(train_scores['roc_auc_ovr_weighted'], test_scores['roc_auc_ovr_weighted'],\n                      'XGBoost', 'learning_rate', learning_rate_range,\n                      'ROC AUC OVR Weighted')\n```\n\n**Output:**\n\n<img src='https://drive.google.com/thumbnail?id=1YhXhf8v3TRdJJku4aPnwUqrSsg13LN-7&sz=w1000' width='1000' alt='XGB Validation Curve for ROC AUC against learning_rate'>\n\n[Click to Open Image](https://drive.google.com/file/d/1YhXhf8v3TRdJJku4aPnwUqrSsg13LN-7/view?usp=drive_link)","metadata":{}},{"cell_type":"markdown","source":"It is evident from the turning point in both the plots that learning_rate=0.05 seems most suitable.\n\nNext, XGBClassifier will be tuned for n_estimators given learning_rate=0.05.","metadata":{}},{"cell_type":"markdown","source":"**Code:**\n```python\n# Tuning XGBClassifier for number of estimators\nxgb = XGBClassifier(objective='multi:softmax', learning_rate=0.05, n_jobs=-1, random_state=random_state)\nn_estimators_range = [10, 20, 50, 80, 100, 150, 200]\n\ntrain_scores, test_scores = multi_metric_validation_curve(xgb, X_train_preprocessed, y_train,\n                                                          param_name='n_estimators', param_range=n_estimators_range,\n                                                          metrics=['neg_log_loss', 'roc_auc_ovr_weighted'],\n                                                          cv=cv)\n```\n\n---\n\n**Code:**\n```python\n# Plot validation curve for Cross-Entropy Loss\nplot_validation_curve(-train_scores['neg_log_loss'], -test_scores['neg_log_loss'],\n                      'XGBoost', 'n_estimators', n_estimators_range,\n                      'Cross-Entropy Loss')\n```\n\n**Output:**\n\n<img src='https://drive.google.com/thumbnail?id=1crIYRxnC3ZjBZBgwEPxjps7CeFXkuwbm&sz=w1000' width='1000' alt='XGB Validation Curve for LogLoss against n_estimators'>\n\n[Click to Open Image](https://drive.google.com/file/d/1crIYRxnC3ZjBZBgwEPxjps7CeFXkuwbm/view?usp=drive_link)\n\n---\n\n**Code:**\n```python\n# Plot validation curve for ROC AUC\nplot_validation_curve(train_scores['roc_auc_ovr_weighted'], test_scores['roc_auc_ovr_weighted'],\n                      'XGBoost', 'n_estimators', n_estimators_range,\n                      'ROC AUC OVR Weighted')\n```\n\n**Output:**\n\n<img src='https://drive.google.com/thumbnail?id=1sePrqD4hijFByBtUfUBfRomP_nM75-jA&sz=w1000' width='1000' alt='XGB Validation Curve for ROC AUC against n_estimators'>\n\n[Click to Open Image](https://drive.google.com/file/d/1sePrqD4hijFByBtUfUBfRomP_nM75-jA/view?usp=drive_link)","metadata":{}},{"cell_type":"markdown","source":"Considering the Cross-Entropy Loss plot, the testing loss starts to increase after n_estimators=100, indicating overfitting. Similarly, for the ROC AUC plot, the metric attains maximum value at n_estimators=100 and remains constant afterwards. Thus, choosing n_estimators=100. \n\nFollowing is tuning for the tree-specific parameter 'max_depth', given learning_rate=0.05 and n_estimators=100.","metadata":{}},{"cell_type":"markdown","source":"**Code:**\n```python\n# Tuning XGBClassifier for max_depth\nxgb = XGBClassifier(objective='multi:softmax', learning_rate=0.05, n_estimators=100, n_jobs=-1, random_state=random_state)\nmax_depth_range = [5, 10, 15, 20]\n\ntrain_scores, test_scores = multi_metric_validation_curve(xgb, X_train_preprocessed, y_train,\n                                                          param_name='max_depth', param_range=max_depth_range,\n                                                          metrics=['neg_log_loss', 'roc_auc_ovr_weighted'],\n                                                          cv=cv)\n```\n\n---\n\n**Code:**\n```python\n# Plot validation curve for Cross-Entropy Loss\nplot_validation_curve(-train_scores['neg_log_loss'], -test_scores['neg_log_loss'],\n                      'XGBoost', 'max_depth', max_depth_range,\n                      'Cross-Entropy Loss')\n```\n\n**Output:**\n\n<img src='https://drive.google.com/thumbnail?id=1paW_uTRsXi-G035FclgyY0J0V-LfoXA0&sz=w1000' width='1000' alt='XGB Validation Curve for LogLoss against max_depth'>\n\n[Click to Open Image](https://drive.google.com/file/d/1paW_uTRsXi-G035FclgyY0J0V-LfoXA0/view?usp=drive_link)\n\n___\n\n**Code:**\n```python\n# Plot validation curve for ROC AUC\nplot_validation_curve(train_scores['roc_auc_ovr_weighted'], test_scores['roc_auc_ovr_weighted'],\n                      'XGBoost', 'max_depth', max_depth_range,\n                      'ROC AUC OVR Weighted')\n```\n\n**Output:**\n\n<img src='https://drive.google.com/thumbnail?id=1kcF0ACC2RaDm-tEYcdpWFJu2N5GznvMi&sz=w1000' width='1000' alt='XGB Validation Curve for ROC AUC against max_depth'>\n\n[Click to Open Image](https://drive.google.com/file/d/1kcF0ACC2RaDm-tEYcdpWFJu2N5GznvMi/view?usp=drive_link)","metadata":{}},{"cell_type":"markdown","source":"Though, Training loss decreases, Testing loss increases post max_depth=5, signalling overfitting.\n\nThus, the tuned classifier has learning_rate=0.05, n_estimators=100, and max_depth=5.","metadata":{}},{"cell_type":"code","source":"# Tuned XGBClassifier\nxgb = XGBClassifier(objective='multi:softmax',\n                    learning_rate=0.05, n_estimators=100, max_depth=5,\n                    n_jobs=-1, random_state=random_state).fit(X_train_preprocessed, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:04:29.059082Z","iopub.execute_input":"2024-04-05T17:04:29.059469Z","iopub.status.idle":"2024-04-05T17:08:01.121449Z","shell.execute_reply.started":"2024-04-05T17:04:29.059434Z","shell.execute_reply":"2024-04-05T17:08:01.120025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute metrics for comparison for XGBClassifier\ncompute_comparison_metrics('XGBoost', xgb)\nresults","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:08:01.123333Z","iopub.execute_input":"2024-04-05T17:08:01.123805Z","iopub.status.idle":"2024-04-05T17:08:01.624779Z","shell.execute_reply.started":"2024-04-05T17:08:01.123755Z","shell.execute_reply":"2024-04-05T17:08:01.622598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='MLP'></a>\n### 5.3.7 Multi-Layer Perceptron (Neural Network)","metadata":{}},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:08:01.626757Z","iopub.execute_input":"2024-04-05T17:08:01.627306Z","iopub.status.idle":"2024-04-05T17:08:01.644809Z","shell.execute_reply.started":"2024-04-05T17:08:01.627254Z","shell.execute_reply":"2024-04-05T17:08:01.643655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Starting with a simple MLP with 1 Input, 2 Hidden, and 1 Output Layers. The number of neurons per hidden is considered as half of that in the previous neuron, i.e. Hidden Layer-1 has 1000 (\\~ # of Features/2 = 2093/2) and Hidden Layer-2 has 500 (\\~1000/2) neurons.\n\nMoreover, after testing with various activation functions, including Sigmoid and Tanh, Rectified Linear Unit (ReLU) for the hidden layers gave the most promising results.\n\n$$\n\\text{ReLU}(z) = \\max(0, z)\n$$\n\nScikit-learn automatically assigns the activation function of the output layer as Softmax for a multi-class classification problem.\n\n$$\n\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{k=1}^{K}{e}^{z_k}}\n$$\n\nThe solver is chosen as 'Adam', which genrally works well on large datasets in terms of training time and validation score. Moreover, early_stopping=True has also been enabled with validation_fraction=0.1 so that model is able to generalize well and prevent overfitting.","metadata":{}},{"cell_type":"code","source":"# Tuning Neural Network for regularization strength\nmlp = MLPClassifier(hidden_layer_sizes=(1000, 500), solver='adam',\n                    early_stopping=True, validation_fraction=0.1, random_state=random_state)\nalpha_range = np.logspace(-4, 1, 6)\n\ntrain_scores, test_scores = multi_metric_validation_curve(mlp, X_train_preprocessed, y_train,\n                                                          param_name='alpha', param_range=alpha_range,\n                                                          metrics=['neg_log_loss', 'roc_auc_ovr_weighted'],\n                                                          cv=cv)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T07:02:15.210233Z","iopub.execute_input":"2024-04-05T07:02:15.211142Z","iopub.status.idle":"2024-04-05T07:11:13.964812Z","shell.execute_reply.started":"2024-04-05T07:02:15.211108Z","shell.execute_reply":"2024-04-05T07:11:13.963054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot validation curve for Cross-Entropy Loss\nplot_validation_curve(-train_scores['neg_log_loss'], -test_scores['neg_log_loss'],\n                      'MLP', 'alpha', alpha_range,\n                      'Cross-Entropy Loss', logx=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T07:11:13.966986Z","iopub.execute_input":"2024-04-05T07:11:13.967845Z","iopub.status.idle":"2024-04-05T07:11:15.614210Z","shell.execute_reply.started":"2024-04-05T07:11:13.967799Z","shell.execute_reply":"2024-04-05T07:11:15.612513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot validation curve for ROC AUC\nplot_validation_curve(train_scores['roc_auc_ovr_weighted'], test_scores['roc_auc_ovr_weighted'],\n                      'MLP', 'alpha', alpha_range,\n                      'ROC AUC OVR Weighted', logx=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T07:11:15.616514Z","iopub.execute_input":"2024-04-05T07:11:15.616956Z","iopub.status.idle":"2024-04-05T07:11:16.773891Z","shell.execute_reply.started":"2024-04-05T07:11:15.616917Z","shell.execute_reply":"2024-04-05T07:11:16.772331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the Cross-Entropy Loss plot, minimum test loss occurs between alpha=0.1 and alpha=1; however, the training loss increases in this range. ROC AUC is also roughly constant till alpha=1 (peaking at alpha=1).\n\nInterestingly, for alpha=10, both train and test losses increase sharply, while ROC AUC values decrease sharply. This suggests underfitting of the MLP.\n\nTherefore, alpha=0.1 is considered ahead as it minimizes the test loss.","metadata":{}},{"cell_type":"code","source":"# Tuned Neural Network\nmlp = MLPClassifier(hidden_layer_sizes=(1000, 500),\n                    solver='adam', alpha=0.1,\n                    early_stopping=True, validation_fraction=0.1,\n                    random_state=random_state).fit(X_train_preprocessed, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:08:01.646730Z","iopub.execute_input":"2024-04-05T17:08:01.647168Z","iopub.status.idle":"2024-04-05T17:10:00.082580Z","shell.execute_reply.started":"2024-04-05T17:08:01.647132Z","shell.execute_reply":"2024-04-05T17:10:00.079898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute metrics for comparison for MLP\ncompute_comparison_metrics('MLP', mlp)\nresults","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:10:00.085194Z","iopub.execute_input":"2024-04-05T17:10:00.086188Z","iopub.status.idle":"2024-04-05T17:10:02.990614Z","shell.execute_reply.started":"2024-04-05T17:10:00.086135Z","shell.execute_reply":"2024-04-05T17:10:02.989228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Activation functions used within layers of the MLP\nprint(f'Hidden Layer Activation: {mlp.get_params()[\"activation\"].title()}')\nprint(f'Output Layer Activation: {mlp.out_activation_.title()}')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:10:02.992255Z","iopub.execute_input":"2024-04-05T17:10:02.996707Z","iopub.status.idle":"2024-04-05T17:10:03.004837Z","shell.execute_reply.started":"2024-04-05T17:10:02.996649Z","shell.execute_reply":"2024-04-05T17:10:03.002995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='Model-Compare'></a>\n## 5.4 Model Comparison","metadata":{}},{"cell_type":"markdown","source":"<a name='Compare-Metrics'></a>\n### 5.4.1 Comparing Metrics","metadata":{}},{"cell_type":"code","source":"# Compiling all trained models\nmodels = {\n    'Logistic Regression': lr,\n    'K Nearest Neighbors': knn,\n    'Support Vector Classifier': svc,\n    'Decision Tree': dt,\n    'Random Forest': rf,\n    'XGBoost': xgb,\n    'Multi-Layer Perceptron': mlp\n}","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:10:03.006519Z","iopub.execute_input":"2024-04-05T17:10:03.006995Z","iopub.status.idle":"2024-04-05T17:10:03.022357Z","shell.execute_reply.started":"2024-04-05T17:10:03.006936Z","shell.execute_reply":"2024-04-05T17:10:03.020795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the Comparison Metrics\nresults","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:10:03.024989Z","iopub.execute_input":"2024-04-05T17:10:03.026167Z","iopub.status.idle":"2024-04-05T17:10:03.066895Z","shell.execute_reply.started":"2024-04-05T17:10:03.026119Z","shell.execute_reply":"2024-04-05T17:10:03.065059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compare Accuracies across Models\nmodel_names = results['Model']\n\ntrain_accuracies = [acc[0] for acc in results['Accuracy']]\ntest_accuracies = [acc[1] for acc in results['Accuracy']]\n\nwindow = (0.75, 0.91)\n\nplt.figure(figsize=(7, 7))\n[plt.scatter(test_accuracies[i], train_accuracies[i]) for i in range(len(models))]\nplt.plot(window, [null_accuracy/100]*2, color='blue', linestyle='dotted')\nplt.plot([null_accuracy/100]*2, window, color='blue', linestyle='dotted')\nplt.plot(window, window, color='black', linestyle='-')\nplt.xlabel('Test Accuracy')\nplt.ylabel('Train Accuracy')\nplt.title('Train vs Test Accuracy')\nplt.legend(list(model_names) + ['Null Accuracy (~0.76)'], loc=\"best\")\nplt.xlim(window)\nplt.ylim(window)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:10:03.068496Z","iopub.execute_input":"2024-04-05T17:10:03.068872Z","iopub.status.idle":"2024-04-05T17:10:03.745770Z","shell.execute_reply.started":"2024-04-05T17:10:03.068839Z","shell.execute_reply":"2024-04-05T17:10:03.744260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As evident, all classifiers (Random Forest barely) are scoring better than a DummyClassifier that predicts the most_frequent class (~0.76 Accuracy).\n\n**Insights:**\n- Random Forest seems as a good a DummyClassifier\n- Decision Tree and KNN perform marginally better than the Null Accuracy\n- Logistic Regression and SVC performs best followed by MLP and XGBoost in terms of test accuracy\n    - **Logistic Regression** performs best (best test accuracy, least overfitting)","metadata":{}},{"cell_type":"code","source":"# Compare Weighted Precisions, Weighted F1 Scores, and Weighted ROC AUC across Models\n# Weighted Recall is same as Accuracy, hence not plotted\n\nfig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16, 5))\n\ntrain_precisions = [pre[0] for pre in results['Precision Weighted']]\ntest_precisions = [pre[1] for pre in results['Precision Weighted']]\n\ntrain_f1 = [f1[0] for f1 in results['F1 Weighted']]\ntest_f1 = [f1[1] for f1 in results['F1 Weighted']]\n\ntrain_roc_auc = [roc_auc[0] for roc_auc in results['ROC AUC OVR Weighted']]\ntest_roc_auc = [roc_auc[1] for roc_auc in results['ROC AUC OVR Weighted']]\n\n[ax[0].scatter(test_precisions[i], train_precisions[i]) for i in range(len(models))]\n[ax[1].scatter(test_f1[i], train_f1[i]) for i in range(len(models))]\n[ax[2].scatter(test_roc_auc[i], train_roc_auc[i]) for i in range(len(models))]\n\n[ax[i].plot((0, 1), (0, 1), color='black', linestyle='-') for i in range(3)]\n\n[[ax[i].set_xlabel(f'Test {metric}'),\n  ax[i].set_ylabel(f'Train {metric}'),\n  ax[i].set_title(f'Train vs Test {metric}')] for (i, metric) in enumerate(['Weighted Precision', 'Weighted F1 Score', 'Weighted ROC AUC'])]\n\nax[0].set_xlim(0.5, 1), ax[0].set_ylim(0.5, 1)\nax[1].set_xlim(0.6, 1), ax[1].set_ylim(0.6, 1)\nax[2].set_xlim(0.6, 1), ax[2].set_ylim(0.6, 1)\n\nax[1].legend(list(model_names), loc=\"best\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:10:03.747932Z","iopub.execute_input":"2024-04-05T17:10:03.748590Z","iopub.status.idle":"2024-04-05T17:10:05.002334Z","shell.execute_reply.started":"2024-04-05T17:10:03.748540Z","shell.execute_reply":"2024-04-05T17:10:05.001154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insights:**\n- Logistic Regression and SVC are the top contenders given the metric graphs.\n    - Train and Test metric values for SVC have a higher deviation than Logistic Regression.\n- MLP and XGBoost also perform relatively well.\n- KNN, Decision Tree, and Random Forest do not perform as well in terms of Weighted Precision and Weighted F1 Scores.\n    - Surprisingly, Random Forest, which lags behind in Precision and F1 Score, performs much better than KNN and Decision Tree in ROC AUC.","metadata":{}},{"cell_type":"markdown","source":"<a name='ROC-Curve'></a>\n### 5.4.2 ROC Curves","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc\n\n# Label Binarize y_val\ny_val_encoded = label_binarize(y_val, classes=[0, 1, 2, 3, 4, 5])\nn_classes = y_val_encoded.shape[1]\n\n# Compute probabilities, tpr, fpr, and roc_auc for each model and plot the ROC curve\nfig, ax = plt.subplots(nrows=2, ncols=3, figsize=(26, 17), sharex=True, sharey=True)\nfor model_name, model in models.items():\n    y_val_pred_proba = model.predict_proba(X_val_preprocessed)\n    for i in range(n_classes):\n        fpr, tpr, threshold = roc_curve(y_val_encoded[:, i], y_val_pred_proba[:, i])\n        roc_auc = auc(fpr, tpr)\n        ax[i//3, i%3].plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})')\n\n[(ax[i//3, i%3].plot([0, 1], [0, 1], 'k--'),\n  ax[i//3, i%3].set_xlabel('False Positive Rate'),\n  ax[i//3, i%3].set_ylabel('True Positive Rate'),\n  ax[i//3, i%3].set_title(f'ROC curve for Class {i}'),\n  ax[i//3, i%3].set_xlim(0, 1),\n  ax[i//3, i%3].set_ylim(0, 1),\n  ax[i//3, i%3].legend(loc='best'),\n  ax[i//3, i%3].grid(True)) for i in range(n_classes)]\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:10:05.004036Z","iopub.execute_input":"2024-04-05T17:10:05.004956Z","iopub.status.idle":"2024-04-05T17:11:14.363110Z","shell.execute_reply.started":"2024-04-05T17:10:05.004921Z","shell.execute_reply":"2024-04-05T17:11:14.361895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The ROC Curves also show that Logistic Regression, SVC, and MLP are the best performing across the classes, while KNN and Decision Trees perform the worst. For classes 0, 4, and 5, almost all classifiers perform similarly.","metadata":{}},{"cell_type":"markdown","source":"**Remarks:**\n- **Logistic Regression** is the best choice of all the models, given its test accuracy and proximity to the $y=x$ line in comparison to SVC.\n- Given the diversity in performance metrics for the models, perhaps an **Ensemble** of these models can be trained and tested as well.","metadata":{}},{"cell_type":"markdown","source":"<a name='Ensemble'></a>\n## 5.5 Ensemble","metadata":{}},{"cell_type":"markdown","source":"Ensemble Learning is a method to combine multiple models to produced improved results. While **Bagging** and **Boosting**, modelled before, only use a single type of model (in this case, a Decision Tree), methods like **Voting** and **Stacking** take advantage of wide variety of models.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier, StackingClassifier","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:11:14.364749Z","iopub.execute_input":"2024-04-05T17:11:14.365922Z","iopub.status.idle":"2024-04-05T17:11:14.371479Z","shell.execute_reply.started":"2024-04-05T17:11:14.365881Z","shell.execute_reply":"2024-04-05T17:11:14.370121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='Voting'></a>\n### 5.5.1 Voting Classifier","metadata":{}},{"cell_type":"markdown","source":"<a name='Hard-Voting'></a>\n#### 5.5.1.1 Hard Voting Classifier\nFinal prediction is based on majority rule voting of predicted classes by the base estimators.","metadata":{}},{"cell_type":"markdown","source":"**Code:**\n```python\nbase_classifiers = list(models.items())\n\nhard_voting_model = VotingClassifier(\n    estimators=base_classifiers,\n    voting='hard',\n    n_jobs=-1\n)\nhard_voting_model.fit(X_train_preprocessed, y_train)\n\nprint(f'Train Accuracy (Hard Voting)     : {hard_voting_model.score(X_train_preprocessed, y_train)}')\nprint(f'Validation Accuracy (Hard Voting): {hard_voting_model.score(X_val_preprocessed, y_val)}')\n```\n\n**Output:**\n```\nTrain Accuracy (Hard Voting)     : 0.8064926175809133\nValidation Accuracy (Hard Voting): 0.769140510413611\n```","metadata":{}},{"cell_type":"markdown","source":"<a name='Soft-Voting'></a>\n#### 5.5.1.2 Soft Voting Classifier\n\nFinal Prediction is the argmax of the sum of prediction probabilities of the base estimators.","metadata":{}},{"cell_type":"code","source":"base_classifiers = list(models.items())\n\nsoft_voting_model = VotingClassifier(\n    estimators=base_classifiers,\n    voting='soft',\n    n_jobs=-1\n)\nsoft_voting_model.fit(X_train_preprocessed, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:11:14.397367Z","iopub.execute_input":"2024-04-05T17:11:14.398266Z","iopub.status.idle":"2024-04-05T17:31:37.567891Z","shell.execute_reply.started":"2024-04-05T17:11:14.398227Z","shell.execute_reply":"2024-04-05T17:31:37.566165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Train Accuracy (Soft Voting)     : {soft_voting_model.score(X_train_preprocessed, y_train)}')\nprint(f'Validation Accuracy (Soft Voting): {soft_voting_model.score(X_val_preprocessed, y_val)}')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:31:37.570287Z","iopub.execute_input":"2024-04-05T17:31:37.570736Z","iopub.status.idle":"2024-04-05T17:36:00.426741Z","shell.execute_reply.started":"2024-04-05T17:31:37.570700Z","shell.execute_reply":"2024-04-05T17:36:00.425417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both the variations of Voting Classifiers portrayed similar results and trained in similar duration of time.\n\nEven though **Hard-Voting Model** has a slightly better Validation Accuracy, the **Soft-Voting Model** is the one to proceed with as it is known to be robust to outliers and can mitigate the bias in an imbalanced dataset because it fundamentally considers probabilities to come to a prediction.","metadata":{}},{"cell_type":"code","source":"# Compute metrics for comparison for Soft-Voting Model\ncompute_comparison_metrics('Soft-Voting', soft_voting_model)\nresults","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:36:00.428661Z","iopub.execute_input":"2024-04-05T17:36:00.429457Z","iopub.status.idle":"2024-04-05T17:45:12.348416Z","shell.execute_reply.started":"2024-04-05T17:36:00.429411Z","shell.execute_reply":"2024-04-05T17:45:12.347107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Soft-Voting Model has decent test Accuracy and weighted F1 Score, yet the best test weighted Precision and ROC AUC value yet.","metadata":{}},{"cell_type":"markdown","source":"<a name='Stacking'></a>\n### 5.5.2 Stacking Classifier\n\nA Meta Estimator (in this case, Logistic Regression) makes a prediction based on the prediction probability outputs of each Base Estimator.","metadata":{}},{"cell_type":"code","source":"base_classifiers = list(models.items())\n\n# Logistic Regression as a Meta Estimator for the Stacking Classifier\n    # Hyperparameters chosen after tuning models across multiple notebooks for penalty='l1' using 'saga' solver and\n    # penalty='l2' using 'lbfgs' solver for C=[0.01, 0.05, 0.1, 1]\n    # Tuning each model took about 1-2 hours\nmeta_classifier = LogisticRegression(C=0.1, penalty='l2', max_iter=5000, n_jobs=-1, random_state=random_state)\n\nstacking_model = StackingClassifier(\n    estimators=base_classifiers,\n    final_estimator=meta_classifier,\n    cv=2,\n    n_jobs=-1,\n    passthrough=True,\n)\nstacking_model.fit(X_train_preprocessed, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:51:14.111596Z","iopub.execute_input":"2024-04-05T17:51:14.112174Z","iopub.status.idle":"2024-04-05T18:23:36.196559Z","shell.execute_reply.started":"2024-04-05T17:51:14.112136Z","shell.execute_reply":"2024-04-05T18:23:36.194554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Train Accuracy (Stacking)     : {stacking_model.score(X_train_preprocessed, y_train)}')\nprint(f'Validation Accuracy (Stacking): {stacking_model.score(X_val_preprocessed, y_val)}')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T18:23:36.200060Z","iopub.execute_input":"2024-04-05T18:23:36.201162Z","iopub.status.idle":"2024-04-05T18:28:12.481288Z","shell.execute_reply.started":"2024-04-05T18:23:36.201099Z","shell.execute_reply":"2024-04-05T18:28:12.479619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute metrics for comparison for Stacking Model\ncompute_comparison_metrics('Stacking', stacking_model)\nresults","metadata":{"execution":{"iopub.status.busy":"2024-04-05T18:28:12.483467Z","iopub.execute_input":"2024-04-05T18:28:12.484341Z","iopub.status.idle":"2024-04-05T18:38:44.091215Z","shell.execute_reply.started":"2024-04-05T18:28:12.484278Z","shell.execute_reply":"2024-04-05T18:38:44.088903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Stacking Model** outperforms all other classifiers in each metric.\n\nThus, Ensemble Learning has provided us with better classifiers than their individual base estimators.","metadata":{}},{"cell_type":"markdown","source":"<a name='Model-Compare-2'></a>\n## 5.6 Model Comparison Part-2\n\nComparing Ensemble classifiers with previously trained models","metadata":{}},{"cell_type":"markdown","source":"<a name='ROC-Curve-2'></a>\n### 5.6.1 ROC Curves","metadata":{}},{"cell_type":"code","source":"# Compiling all trained models\nmodels = {\n    'Logistic Regression': lr,\n    'K Nearest Neighbors': knn,\n    'Support Vector Classifier': svc,\n    'Decision Tree': dt,\n    'Random Forest': rf,\n    'XGBoost': xgb,\n    'Multi-Layer Perceptron': mlp,\n    'Soft-Voting': soft_voting_model,\n    'Stacking': stacking_model\n}","metadata":{"execution":{"iopub.status.busy":"2024-04-05T18:38:44.096965Z","iopub.execute_input":"2024-04-05T18:38:44.097609Z","iopub.status.idle":"2024-04-05T18:38:44.106993Z","shell.execute_reply.started":"2024-04-05T18:38:44.097554Z","shell.execute_reply":"2024-04-05T18:38:44.105075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label Binarize y_val\ny_val_encoded = label_binarize(y_val, classes=[0, 1, 2, 3, 4, 5])\nn_classes = y_val_encoded.shape[1]\n\n# Compute probabilities, tpr, fpr, and roc_auc for each model and plot the ROC curve\nfig, ax = plt.subplots(nrows=2, ncols=3, figsize=(26, 17), sharex=True, sharey=True)\nfor model_name, model in models.items():\n    y_val_pred_proba = model.predict_proba(X_val_preprocessed)\n    for i in range(n_classes):\n        fpr, tpr, threshold = roc_curve(y_val_encoded[:, i], y_val_pred_proba[:, i])\n        roc_auc = auc(fpr, tpr)\n        ax[i//3, i%3].plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})')\n\n[(ax[i//3, i%3].plot([0, 1], [0, 1], 'k--'),\n  ax[i//3, i%3].set_xlabel('False Positive Rate'),\n  ax[i//3, i%3].set_ylabel('True Positive Rate'),\n  ax[i//3, i%3].set_title(f'ROC curve for Class {i}'),\n  ax[i//3, i%3].set_xlim(0, 1),\n  ax[i//3, i%3].set_ylim(0, 1),\n  ax[i//3, i%3].legend(loc='best'),\n  ax[i//3, i%3].grid(True)) for i in range(n_classes)]\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T18:38:44.109749Z","iopub.execute_input":"2024-04-05T18:38:44.110292Z","iopub.status.idle":"2024-04-05T18:42:25.588802Z","shell.execute_reply.started":"2024-04-05T18:38:44.110246Z","shell.execute_reply":"2024-04-05T18:42:25.587078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ensemble models (Soft-Voting and Stacking) have the highest AUC value for almost all the classes.\n\nThus, the ROC Curves strengthen the claim that Ensemble classifiers take advantage of their base estimators to enhance their predictive capabilities.","metadata":{}},{"cell_type":"markdown","source":"<a name='Confusion-Error-Matrix'></a>\n### 5.6.2 Confusion and Error Matrices","metadata":{}},{"cell_type":"code","source":"# Computing Confusion Matrix for Stacking, Soft-Voting, and Logistic Regression (Top-3 Models)\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ny_val_pred = stacking_model.predict(X_val_preprocessed)\nconf_mat_stacking = confusion_matrix(y_val, y_val_pred)\n\ny_val_pred = soft_voting_model.predict(X_val_preprocessed)\nconf_mat_soft_voting = confusion_matrix(y_val, y_val_pred)\n\ny_val_pred = lr.predict(X_val_preprocessed)\nconf_mat_lr = confusion_matrix(y_val, y_val_pred)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:06:01.315873Z","iopub.execute_input":"2024-04-05T19:06:01.316408Z","iopub.status.idle":"2024-04-05T19:08:16.782914Z","shell.execute_reply.started":"2024-04-05T19:06:01.316358Z","shell.execute_reply":"2024-04-05T19:08:16.781027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rendering the Confusion Matrix\nfig, ax = plt.subplots(nrows=1, ncols=3, figsize=(22, 5))\n\nConfusionMatrixDisplay(conf_mat_stacking, display_labels=[f'Class {i}' for i in range(n_classes)]).plot(ax=ax[0])\nConfusionMatrixDisplay(conf_mat_soft_voting, display_labels=[f'Class {i}' for i in range(n_classes)]).plot(ax=ax[1])\nConfusionMatrixDisplay(conf_mat_lr, display_labels=[f'Class {i}' for i in range(n_classes)]).plot(ax=ax[2])\n\nax[0].set_title('Confusion Matrix for Stacking Model')\nax[1].set_title('Confusion Matrix for Soft-Voting Model')\nax[2].set_title('Confusion Matrix for Logistic Regression Model')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:14:24.590602Z","iopub.execute_input":"2024-04-05T19:14:24.591097Z","iopub.status.idle":"2024-04-05T19:14:25.707329Z","shell.execute_reply.started":"2024-04-05T19:14:24.591057Z","shell.execute_reply":"2024-04-05T19:14:25.705593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Confusion Matrix are not very informative because the class imbalance skews the color gradient scale. Rather, Error Matrices are plotted ahead to understand certain model behavior.\n\n> **Note:** For the purposes of model comparison, in this notebook an Error Matrix is defined as a way to respresent the Confusion Matrix without the correct classifications, thus highlighting the errors in prediction.","metadata":{}},{"cell_type":"code","source":"# Computing and Rendering Error Matrics for Stacking, Soft-Voting and Logistic Regression\nerror_mat_stacking = np.copy(conf_mat_stacking)\nerror_mat_soft_voting = np.copy(conf_mat_soft_voting)\nerror_mat_lr = np.copy(conf_mat_lr)\nfor i in range(n_classes):\n    error_mat_stacking[i][i] = 0\n    error_mat_soft_voting[i][i] = 0\n    error_mat_lr[i][i] = 0\n\nfig, ax = plt.subplots(nrows=1, ncols=3, figsize=(22, 5))\n\nConfusionMatrixDisplay(error_mat_stacking, display_labels=[f'Class {i}' for i in range(n_classes)]).plot(ax=ax[0])\nConfusionMatrixDisplay(error_mat_soft_voting, display_labels=[f'Class {i}' for i in range(n_classes)]).plot(ax=ax[1])\nConfusionMatrixDisplay(error_mat_lr, display_labels=[f'Class {i}' for i in range(n_classes)]).plot(ax=ax[2])\n\nax[0].set_title('Error Matrix for Stacking Model')\nax[1].set_title('Error Matrix for Soft-Voting Model')\nax[2].set_title('Error Matrix for Logistic Regression Model')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:20:33.019120Z","iopub.execute_input":"2024-04-05T19:20:33.020667Z","iopub.status.idle":"2024-04-05T19:20:34.510111Z","shell.execute_reply.started":"2024-04-05T19:20:33.020599Z","shell.execute_reply":"2024-04-05T19:20:34.508800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above matrices highlight that all top-3 models are making significant **Type-1 Errors** (False Positives) over Type-2 Errors (False Negatives), especially for actual classes 0 and 4 against predicted class 5.\n\n- For an Actual Rating of 4 and a Predicted Rating of 5, these errors might be plausible as certain features are likely to overlap between a 'Good' and a 'Very Good' recipe review.\n\n- For an Actual Rating of 0 and a Predicted Rating of 5, a likely scenario that might justify these errors to occur is if users tend to write **_'great'_** reviews for a recipe (causing the model to predict a Rating of 5), yet **_'forget'_** to submit an actual rating (Actual Rating entered as 0 into the system).\n    - Ways to mitigate this have been discussed in [6.2 Closing Remarks Further Scope](#Close-Scope) ahead.","metadata":{}},{"cell_type":"markdown","source":"<a name='Conclusion'></a>\n# 6. Conclusion","metadata":{}},{"cell_type":"markdown","source":"<a name='Final-Model'></a>\n## 6.1 Final Model and Generating Submission File\n\nBased on the metrics and ROC Curves, the top 3 performing classifiers are:\n\n1. Stacking Classifier\n2. Soft-Voting Classifier\n3. Logistic Regression Classifier","metadata":{}},{"cell_type":"code","source":"# Final Model Pipeline\nmodel = Pipeline([('preprocessor', preprocessor),\n#                   ('classifier', lr)\n#                   ('classifier', soft_voting_model)\n                  ('classifier', stacking_model)\n])\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-04-05T18:53:49.567272Z","iopub.execute_input":"2024-04-05T18:53:49.568857Z","iopub.status.idle":"2024-04-05T18:53:52.701481Z","shell.execute_reply.started":"2024-04-05T18:53:49.568775Z","shell.execute_reply":"2024-04-05T18:53:52.700096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train Model on the entire train.csv dataset\nX_combined = pd.concat([X_train, X_val])\ny_combined = pd.concat([y_train, y_val])\nmodel.fit(X_combined, y_combined)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:28:48.018237Z","iopub.execute_input":"2024-04-05T19:28:48.018863Z","iopub.status.idle":"2024-04-05T20:26:19.617875Z","shell.execute_reply.started":"2024-04-05T19:28:48.018816Z","shell.execute_reply":"2024-04-05T20:26:19.616539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating submission.csv\nX_test = pd.read_csv('/kaggle/input/recipe-for-rating-predict-food-ratings-using-ml/test.csv')\n\ny_test_pred = model.predict(X_test)\n\nsubmission = pd.DataFrame({\n    'ID': range(1, 4547),\n    'Rating': y_test_pred\n})\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T20:26:19.620570Z","iopub.execute_input":"2024-04-05T20:26:19.621716Z","iopub.status.idle":"2024-04-05T20:28:31.298292Z","shell.execute_reply.started":"2024-04-05T20:26:19.621674Z","shell.execute_reply":"2024-04-05T20:28:31.296512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='Close-Scope'></a>\n## 6.2 Closing Remarks and Further Scope\n\n","metadata":{}},{"cell_type":"markdown","source":"The top-3 models, namely Stacking, Soft-Voting and Logistic Regression, portray a satisfactory test accuracy (~78%) given the dataset and constraints on library usage.","metadata":{}},{"cell_type":"markdown","source":"#### **Handling Imbalance and Outliers**\nThough, various efforts have been made to diminish the impact of the **imbalanced** dataset as well as **outliers**, I believe the data can be further experimented with and refined, perhaps\n- using techniques like `SMOTETomek`, from the _imblearn_ library, to apply a mixture of **Oversampling** (SMOTE) and **Undersampling** (Tomek Links) in order to rebalance the dataset.\n- using certain **Unsupervised** algorithms like `IsolationForest` or `LocalOutlierFactor` to better detect outliers.\n- using Feature Selection techniques such as Recursive Feature Elimination (RFE) or Sequential Feature Selection (SFS) could help remove noisy variables and further mitigate the chances of fitting the noise. ","metadata":{}},{"cell_type":"markdown","source":"#### **Improved Text Processing**\nI am satisfied with the custom preprocessor developed for the Tf-Idf vectorization given the library constraints. Nevertheless, beyond this Kaggle competition, I believe various suites of libraries that have been already developed from years of text processing experience and much more massive datasets could benefit this model.\n- Libraries like _Natural Language Toolkit_ (_nltk_) might have better preprocessors, tokenizers and even **lemmatizers** for the purpose. _Nltk_ also has more extensive set of stop words than _Scikit-Learn_ and could be beneficial in that regard as stopwords are being used to vectorize the 'Recipe_Review' feature.\n- Libraries like _TextBlob_ and _SpaCy_ might be beneficial as they can be employed to perform **Sentiment Analysis** and arrive at 'Polarity' and 'Subjectivity' scores.\n- Tools like _OpenRefine_ and other Lemmatization techniques can help in refining the tokenized words.\n    - For e.g., currently words like `['zucchini', 'zucchine', 'zuchhini', 'zuchinni']` are being treated as separate words despite the fact that they refer to the same entity and are just spelling errors).","metadata":{}},{"cell_type":"markdown","source":"#### **Deep Learning Approach**\nGiven the fact that the MLP model was among the top-5 classifiers, exploring deep-learning libraries like _TensorFlow_ and _Keras_ might be advantageous in that regard. These APIs could be used to develop an Auto-Encoder which usually is a better alternative to SVD. Also, various other activation functions like 'Scaled Exponential Linear Unit' (SELU), which are not available in the _Scikit-Learn_, could be utilised. Overall, _TensorFlow_ and _Keras_ will provide better flexibility to code each individual hidden layer of a neural network.","metadata":{}},{"cell_type":"markdown","source":"#### **Tackling Misclassification Errors and Handling 0 Rating**\nBeyond data processing, from the misclassifications in the Error Matrices, it is significant that class 0 and 5 have overlapping features. The reason behind this, as already stated, could be writing 'great' reviews but not manually entering a rating. 0 as a Rating is not part of the 1-5 scale since 0 highlights unrated. Ideally, 0 ratings should not be considered while training such a model and only a value from 1-5 should be outputted; however, the competition requires us to predict 0's as well.\n\nThere are 2 ways to solve this:\n\n1. Consider 0 as a separate class and perform multi-class classification (Approach followed in this notebook)\n\n2. Consider the entire problem as a multi-output multi-class problem with 2 target values to be predicted\n    - First model predicts the ideal rating in the range 1-5 (Ordinal target could employ both Regression and Classification techniques)\n    - Second model predicts if the review will be ultimately rated or not\n    - Considering both these predicted targets or their probabilities, a final prediction could made in a way similar to how a Stacking Classifier works. For e.g.,\n        - If the first model predicts a Rating of 5 and the second model predicts that the review will be rated, then the final prediction could be 5\n        - However, if the first model predicts a Rating of 5, yet the second model predicts that the review will not be rated, then the final prediction could be 0","metadata":{}},{"cell_type":"markdown","source":"#### **Closing Remarks**\nOverall, the entire journey of a developing ML model to predict Recipe Review Ratings has been a challenging yet rewarding one. It was an exciting process to develop ML models in a competitive environment that always fuels one to think of out-of-the-box methods to approach problems. All in all, I am extremely pleased with what I have managed to create and this competition, being my first, has acted as an impetus for me to participate in many more.","metadata":{}}]}